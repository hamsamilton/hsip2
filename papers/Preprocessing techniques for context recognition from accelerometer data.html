<!DOCTYPE HTML>
<html lang="en-gb" class="no-js">
    <head>
        <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
<meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=2.5,user-scalable=yes">
    <meta name="citation_publisher" content="Springer-Verlag"/>
    <meta name="citation_title" content="Preprocessing techniques for context recognition from accelerometer data"/>
    <meta name="citation_doi" content="10.1007/s00779-010-0293-9"/>
    <meta name="citation_language" content="en"/>
    <meta name="citation_abstract_html_url" content="https://link.springer.com/article/10.1007/s00779-010-0293-9"/>
    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s00779-010-0293-9"/>
    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007%2Fs00779-010-0293-9.pdf"/>
    <meta name="citation_springer_api_url" content="http://api.springer.com/metadata/pam?q=doi:10.1007/s00779-010-0293-9&amp;api_key="/>
    <meta name="citation_firstpage" content="645"/>
    <meta name="citation_lastpage" content="662"/>
    <meta name="citation_author" content="Davide Figo"/>
    <meta name="citation_author_institution" content="IST, Technical University of Lisbon"/>
    <meta name="citation_author" content="Pedro C. Diniz"/>
    <meta name="citation_author_institution" content="IST, Technical University of Lisbon"/>
    <meta name="citation_author" content="Diogo R. Ferreira"/>
    <meta name="citation_author_institution" content="IST, Technical University of Lisbon"/>
    <meta name="citation_author_email" content="diogo.ferreira@ist.utl.pt"/>
    <meta name="citation_author" content="JoÃ£o M. P. Cardoso"/>
    <meta name="citation_author_institution" content=" University of Porto (FEUP)"/>
    <meta name="dc.identifier" content="10.1007/s00779-010-0293-9"/>
    <meta name="format-detection" content="telephone=no"/>
    <meta name="description" content="The ubiquity of communication devices such as smartphones has led to the emergence of context-aware services that are able to respond to specific user activities or contexts. These services allow..."/>
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:title" content="Preprocessing techniques for context recognition from accelerometer da"/>
    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/779/14/7.jpg"/>
    <meta name="twitter:image:alt" content="Content cover image"/>
    <meta name="twitter:site" content="SpringerLink"/>
    <meta name="twitter:description" content="The ubiquity of communication devices such as smartphones has led to the emergence of context-aware services that are able to respond to specific user activities or contexts. These services allow..."/>
    <meta name="citation_journal_title" content="Personal and Ubiquitous Computing"/>
    <meta name="citation_journal_abbrev" content="Pers Ubiquit Comput"/>
    <meta name="citation_volume" content="14"/>
    <meta name="citation_issue" content="7"/>
    <meta name="citation_issn" content="1617-4909"/>
    <meta name="citation_issn" content="1617-4917"/>
    <meta name="citation_online_date" content="2010/03/30"/>
    <meta name="citation_cover_date" content="2010/10/01"/>
    <meta name="citation_article_type" content="Original Article"/>
    <meta property="og:title" content="Preprocessing techniques for context recognition from accelerometer data"/>
    <meta property="og:type" content="Article"/>
    <meta property="og:url" content="https://link.springer.com/article/10.1007/s00779-010-0293-9"/>
    <meta property="og:image" content="https://static-content.springer.com/cover/journal/779/14/7.jpg"/>
    <meta property="og:site_name" content="SpringerLink"/>
    <meta property="og:description" content="The ubiquity of communication devices such as smartphones has led to the emergence of context-aware services that are able to respond to specific user activities or contexts. These services allow..."/>

        <title>Preprocessing techniques for context recognition from accelerometer data | SpringerLink</title>
        <link rel="canonical" href="https://link.springer.com/article/10.1007/s00779-010-0293-9"/>
        <link rel="shortcut icon" href="/springerlink-static/632953562/images/favicon/favicon.ico">
<link rel="icon" sizes="16x16 32x32 48x48" href="/springerlink-static/632953562/images/favicon/favicon.ico">
<link rel="icon" sizes="16x16" type="image/png" href="/springerlink-static/632953562/images/favicon/favicon-16x16.png">
<link rel="icon" sizes="32x32" type="image/png" href="/springerlink-static/632953562/images/favicon/favicon-32x32.png">
<link rel="icon" sizes="48x48" type="image/png" href="/springerlink-static/632953562/images/favicon/favicon-48x48.png">
<link rel="apple-touch-icon" href="/springerlink-static/632953562/images/favicon/app-icon-iphone@3x.png">
<link rel="apple-touch-icon" sizes="72x72" href="/springerlink-static/632953562/images/favicon/ic_launcher_hdpi.png">
<link rel="apple-touch-icon" sizes="76x76" href="/springerlink-static/632953562/images/favicon/app-icon-ipad.png">
<link rel="apple-touch-icon" sizes="114x114" href="/springerlink-static/632953562/images/favicon/app-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/springerlink-static/632953562/images/favicon/app-icon-iphone@2x.png">
<link rel="apple-touch-icon" sizes="144x144" href="/springerlink-static/632953562/images/favicon/ic_launcher_xxhdpi.png">
<link rel="apple-touch-icon" sizes="152x152" href="/springerlink-static/632953562/images/favicon/app-icon-ipad@2x.png">
<link rel="apple-touch-icon" sizes="180x180" href="/springerlink-static/632953562/images/favicon/app-icon-iphone@3x.png">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/springerlink-static/632953562/images/favicon/ic_launcher_xxhdpi.png">
        <link rel="dns-prefetch" href="//fonts.gstatic.com">
<link rel="dns-prefetch" href="//fonts.googleapis.com">
<link rel="dns-prefetch" href="//google-analytics.com">
<link rel="dns-prefetch" href="//www.google-analytics.com">
<link rel="dns-prefetch" href="//www.googletagservices.com">
<link rel="dns-prefetch" href="//www.googletagmanager.com">
<link rel="dns-prefetch" href="//static-content.springer.com">
        <link rel="stylesheet" href="/springerlink-static/632953562/css/basic.css" media="screen">
<link rel="stylesheet" href="/springerlink-static/632953562/css/styles.css" class="js-ctm" media="only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)">
<link rel="stylesheet" href="/springerlink-static/632953562/css/print.css" media="print">


            <script type="text/javascript">
        window.Krux||((Krux=function(){Krux.q.push(arguments);}).q=[]);
        var dataLayer = [{
                'GA Key':"UA-26408784-1",
                'Features':["leaderboardadverts","eventtracker"],
                'Event Category':"Article",
                'Open Access':"N",
                'Labs':"Y",
                'DOI':"10.1007/s00779-010-0293-9",
                'VG Wort Identifier':"pw-vgzm.415900-10.1007-s00779-010-0293-9",
                'hasAccess':"Y",
                'Full HTML':"Y",
                'Has Body':"Y",
                'Static Hash':"632953562",
                'Has Preview':"N",
                'user':{"license":{"businessPartnerID":["1600000022","3000118102","3000133814"],"businessPartnerIDString":"1600000022|3000118102|3000133814"}},
                'content':{"serial":{"eissn":"1617-4917","pissn":"1617-4909"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"User Interfaces and Human Computer Interaction","2":"Computer Science, general","3":"Personal Computing","4":"Mobile Computing"},"secondarySubjectCodes":{"1":"I18067","2":"I00001","3":"I24083","4":"I29060"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"bunsen"}},
                'Access Type':"subscription",
                'Page':"article",
                'Bpids':"1600000022, 3000118102, 3000133814",
                'Bpnames':"Northwestern University, Northwestern University, Big Ten Academic Alliance Formerly (CIC)",
                'SubjectCodes':"SCI, SCI18067, SCI00001, SCI24083, SCI29060",
                'session':{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},
                'eventTrackerBaseUrl':"https://event-tracker.springernature.com",
                'Keywords':"Activity detection, Context-aware applications, Mobile computing, Sensor data",
                'Country':"US",
                'Journal Id':"779",
                'Journal Title':"Personal and Ubiquitous Computing",

                    'doi': "10.1007-s00779-010-0293-9",
                    'kwrd': ["Activity_detection","Context-aware_applications","Mobile_computing","Sensor_data"],
                    'pmc': ["I","I18067","I00001","I24083","I29060"],
                    'BPID': ["1600000022","3000118102","3000133814"],
                    'ksg': Krux.segments,
                    'kuid': Krux.uid,

        }];
    </script>

<script type="text/javascript" src="/springerlink-static/632953562/js/jquery-3.3.1.min.js"></script>

        <script type="text/javascript" src="https://cdn.cookielaw.org/consent/6b2ec9cd-5ace-4387-96d2-963e596401c6.js" charset="UTF-8"></script>

<script type="text/javascript">
    function OptanonWrapper() {
        dataLayer.push({
            'event' : 'onetrustActive'
        });
    }
</script>

    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
                new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
            j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
            'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-WCF9Z9');</script>

    </head>
    <body>
        <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-WCF9Z9"
                      height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

    <div class="skip-to">
    <a class="skip-to__link pseudo-focus" href="#main-content">Skip to main content</a>
        <a class="skip-to__link skip-to__link--contents pseudo-focus" href="#article-contents">Skip to sections</a>
</div>
        <div class="page-wrapper">
            <noscript>
    <div class="nojs-banner u-interface">
        <p>This service is more advanced with JavaScript available, learn more at <a
                href="http://activatejavascript.org" target="_blank" rel="noopener">http://activatejavascript.org</a>
        </p>
    </div>
</noscript>
                        <div id="leaderboard" class="leaderboard u-hide" data-component="SpringerLink.GoogleAds" data-namespace="leaderboard">
            <div class="leaderboard__wrapper">
                <p class="leaderboard__label">Advertisement</p>
                <button class="leaderboard__hide" title="Hide this advertisement"  data-track="click" data-track-action="Hide advertisement" data-track-label="">Hide</button>
                <div id="doubleclick-leaderboard-ad" class="leaderboard__ad u-pt-24"></div>
            </div>
        </div>


                <header id="header" class="header u-interface">
        <div class="header__content">
            <div class="header__menu-container">
                    <a id="logo" class="site-logo" href="/" title="Go to homepage">
                <div class="u-screenreader-only">SpringerLink</div>
    <svg class="site-logo__springer" width="148" height="30" role="img" focusable="false" aria-hidden="true">
        <image width="148" height="30" alt="" src="/springerlink-static/632953562/images/png/springerlink.png" xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="/springerlink-static/632953562/images/svg/springerlink.svg"></image>
    </svg>

    </a>


                    <nav id="search-container" class="u-inline-block">
                        <div class="search">
                            <div class="search__content">
                                <form class="u-form-single-input" action="/search" method="get" role="search">
    <label for="search-springerlink">Search SpringerLink</label>
    <div class="u-relative">
        <input id="search-springerlink" name="query" type="text" autocomplete="off" value="">
        <input class="u-hide-text" type="submit" value="Submit" title="Submit">
        <svg class="u-vertical-align-absolute" width="13" height="13" viewBox="222 151 13 13" version="1.1" xmlns="http://www.w3.org/2000/svg" focusable="false" aria-hidden="true" role="presentation">
            <path d="M227 159C228.7 159 230 157.7 230 156 230 154.3 228.7 153 227 153 225.3 153 224 154.3 224 156 224 157.7 225.3 159 227 159L227 159 227 159 227 159ZM230 160.1L231.1 159 233.9 161.7C234.2 162.1 234.2 162.6 233.9 162.9 233.6 163.2 233.1 163.2 232.7 162.9L230 160.1 230 160.1 230 160.1 230 160.1ZM227 161L227 161C224.2 161 222 158.8 222 156 222 153.2 224.2 151 227 151 229.8 151 232 153.2 232 156 232 158.8 229.8 161 227 161L227 161 227 161 227 161 227 161Z" stroke="none" fill-rule="evenodd"/>
        </svg>
    </div>
</form>
                            </div>
                        </div>
                    </nav>

                    <nav class="nav-container u-interface">
    <div class="global-nav__wrapper">
        <div class="search-button">
            <a class="search-button__label" href="#search-container">
                <span class="search-button__title">Search</span><svg width="12" height="12" viewBox="222 151 12 12" version="1.1" xmlns="http://www.w3.org/2000/svg" focusable="false" aria-hidden="true" role="presentation">
                    <path d="M227 159C228.7 159 230 157.7 230 156 230 154.3 228.7 153 227 153 225.3 153 224 154.3 224 156 224 157.7 225.3 159 227 159L227 159 227 159 227 159ZM230 160.1L231.1 159 233.9 161.7C234.2 162.1 234.2 162.6 233.9 162.9 233.6 163.2 233.1 163.2 232.7 162.9L230 160.1 230 160.1 230 160.1 230 160.1ZM227 161L227 161C224.2 161 222 158.8 222 156 222 153.2 224.2 151 227 151 229.8 151 232 153.2 232 156 232 158.8 229.8 161 227 161L227 161 227 161 227 161 227 161Z" stroke="none" fill-rule="evenodd"></path>
                </svg>
            </a>
        </div>

        <ul class="global-nav" data-component="SV.Menu" data-title="Navigation menu" data-text="Menu">
            <li>
                <a href="/">
                    <span class="u-overflow-ellipsis">Home</span>
                </a>
            </li>

                <li class="global-nav__logged-out">
                    <a class="test-login-link" href="//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs00779-010-0293-9">
                        <span class="u-overflow-ellipsis">Log in</span>
                    </a>
                </li>

        </ul>
    </div> 
</nav> 
            </div>

        </div>
    </header>

            
            <main id="main-content" class="main-wrapper" tabindex="-1">
                <div class="main-container uptodate-recommendations-off">
                    <aside class="main-sidebar-left">
                        <div class="main-sidebar-left__content">
                            <div class="cover-image test-cover" itemscope>
                                    <a class="test-cover-link" href="/journal/779">
        <span class="u-screenreader-only">Personal and Ubiquitous Computing</span>
        <img class="test-cover-image" src="https://media.springernature.com/w306/springer-static/cover/journal/779/14/7.jpg" itemprop="image" alt=""/>
    </a>


                            </div>
                        </div>
                    </aside>
                    <div class="main-body" data-role="NavigationContainer">
                                <div class="cta-button-container cta-button-container--top cta-button-container--stacked u-mb-16 u-hide-two-col">
                    <div>
            <a href="/content/pdf/10.1007%2Fs00779-010-0293-9.pdf" target="_blank" class="c-button c-button--blue c-button__icon-right" title="Download this article in PDF format" rel="noopener" data-track="click" data-track-action="Pdf download" data-track-label="">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" version="1.1"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill="#fff"><g transform="translate(12.000000, 5.000000)"><path d="M7 7.3L7 1C7 0.4 6.6 0 6 0 5.4 0 5 0.4 5 1L5 7.3 3.5 5.7C3.1 5.3 2.5 5.3 2.1 5.7L2.1 5.7C1.7 6.1 1.7 6.7 2.1 7.1L5.3 10.3C5.7 10.7 6.3 10.7 6.7 10.3L9.9 7.1C10.3 6.7 10.3 6.1 9.9 5.7L9.9 5.7C9.5 5.3 8.9 5.3 8.5 5.7L7 7.3 7 7.3ZM0 13C0 12.4 0.5 12 1 12L11 12C11.6 12 12 12.4 12 13 12 13.6 11.5 14 11 14L1 14C0.4 14 0 13.6 0 13L0 13Z"/></g></g></g></svg>
                <span class="hide-text-small">Download</span>
                <span>PDF</span>
            </a>
        </div>

        </div>



                        <article class="main-body__content">
                            <div xmlns="http://www.w3.org/1999/xhtml" class="FulltextWrapper"><div class="ArticleHeader main-context"><div id="enumeration" class="enumeration"><p><a href="/journal/779" title="Personal and Ubiquitous Computing" data-track="click" data-track-action="Journal title" data-track-label=""><span class="JournalTitle">Personal and Ubiquitous Computing</span></a></p><p class="icon--meta-keyline"><span class="ArticleCitation_Year"><time datetime="2010-10">October 2010</time>, </span><span class="ArticleCitation_Volume">Volume 14, </span><a class="ArticleCitation_Issue" href="/journal/779/14/7/page/1" data-track="click" data-track-action="Article issue" data-track-label="">IssueÂ 7</a>,
                       <span class="ArticleCitation_Pages"> pp 645â662</span><span class="u-inline-block u-ml-4"> | <a href="#citeas" data-track="click" data-track-action="Cite as link" data-track-label="Enumeration section">Cite as</a></span></p></div><div class="MainTitleSection"><h1 class="ArticleTitle" lang="en">Preprocessing techniques for context recognition from accelerometer data</h1></div><div class="authors u-clearfix" data-component="SpringerLink.Authors"><ul class="u-interface u-inline-list authors__title" data-role="AuthorsNavigation"><li><span>Authors</span></li><li><a href="#authorsandaffiliations" data-track="click" data-track-action="Authors and affiliations tab" data-track-label="">Authors and affiliations</a></li></ul><div class="authors__list" data-role="AuthorsList"><ul class="test-contributor-names"><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">DavideÂ Figo</span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">PedroÂ C.Â Diniz</span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">DiogoÂ R.Â Ferreira</span><span class="author-information"><span class="authors__contact"><a href="mailto:diogo.ferreira@ist.utl.pt" title="diogo.ferreira@ist.utl.pt" itemprop="email" data-track="click" data-track-action="Email author" data-track-label=""><img src="/springerlink-static/images/svg/email.svg" height="24" width="24" alt="Email author" /></a></span></span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors__name">JoÃ£oÂ M.Â P.Â Cardoso</span></li></ul></div></div><div class="main-context__container" data-component="SpringerLink.ArticleMetrics"><div class="main-context__column"><span><span class="test-render-category">Original Article</span></span><div class="article-dates"><span class="article-dates__label">First Online: </span><span class="article-dates__first-online"><time datetime="2010-03-30">30 March 2010</time></span></div></div><div class="main-context__column">    <ul id="book-metrics" class="article-metrics u-sansSerif">
            <li class="article-metrics__item">
                    <a class="article-metrics__link gtm-socialmediamentions-count" href="http://www.altmetric.com/details.php?citation_id&#x3D;2761251&amp;domain&#x3D;link.springer.com" target="_blank" rel="noopener"
                       title="Visit Altmetric for full social mention details" id="socialmediamentions-link">
                            <span id="socialmediamentions-count-number" class="test-metric-count c-button-circle gtm-socialmediamentions-count">1</span>
                       <span class="test-metric-name article-metrics__label gtm-socialmediamentions-count">Shares</span>
                    </a>
            </li>
            <li class="article-metrics__item">
                     <span class="article-metrics__views">4.1k</span>
                     <span class="article-metrics__label">Downloads</span>
            </li>
            <li class="article-metrics__item">
                    <a class="article-metrics__link gtm-citations-count" href="https://citations.springer.com/item?doi&#x3D;10.1007/s00779-010-0293-9" target="_blank" rel="noopener"
                       title="Visit Springer Citations for full citation details" id="citations-link">
                            <span id="citations-count-number" class="test-metric-count c-button-circle gtm-citations-count">247</span>
                       <span class="test-metric-name article-metrics__label gtm-citations-count">Citations</span>
                    </a>
            </li>
    </ul>
</div></div></div><section class="Abstract" id="Abs1" tabindex="-1" lang="en"><h2 class="Heading">Abstract</h2><p class="Para">The ubiquity of communication devices such as smartphones has led to the emergence of context-aware services that are able to respond to specific user activities or contexts. These services allow communication providers to develop new, added-value services for a wide range of applications such as social networking, elderly care and near-emergency early warning systems. At the core of these services is the ability to detect specific physical settings or the context a user is in, using either internal or external sensors. For example, using built-in accelerometers, it is possible to determine whether a user is walking or running at a specific time of day. By correlating this knowledge with GPS data, it is possible to provide specific information services to users with similar daily routines. This article presents a survey of the techniques for extracting this activity information from raw accelerometer data. The techniques that can be implemented in mobile devices range from classical signal processing techniques such as FFT to contemporary string-based methods. We present experimental results to compare and evaluate the accuracy of the various techniques using real data sets collected from daily activities.</p></section><div class="KeywordGroup" lang="en"><h2 class="Heading">Keywords</h2><span class="Keyword">Activity detectionÂ </span><span class="Keyword">Context-aware applicationsÂ </span><span class="Keyword">Mobile computingÂ </span><span class="Keyword">Sensor dataÂ </span></div><div class="article-actions--inline" id="article-actions--inline" data-component="article-actions--inline"></div><div id="body"><section id="Sec1" tabindex="-1" class="Section1 RenderAsSection1 SectionTypeIntroduction"><h2 class="Heading"><span class="HeadingNumber">1 </span>Introduction</h2><div class="content"><p class="Para">Mobile communication devices as the ubiquitous cellular phones, and more recently smartphones, have exploded in number and computing capabilities in recent years. Rather than supporting only voice communications, contemporary devices have sophisticated internal hardware architectures and possibly also an extended range of functions such a GPS location, e-mail, organizer and synchronization with external, often centralized services. Advanced units can even be equipped with a wide range of internal sensors including three-dimensional accelerometers as well as the ability to interface with external web-based sensor services such as traffic information.</p><p class="Para">Using sensor data, mobile devices can provide users with an wide range of added-value services. For example, by analyzing accelerometer data, a device can understand that the user is performing some physical activity such as walking or running. This knowledge can be gathered over a period of time, say a week or even a month, to recognize trends or daily habits. Knowing that at a specific time of day a user might be jogging at a specific location, it is possible to send a message advertising a refreshment booth or advertising a specific brand of running shoes.
<sup><a href="#Fn1" id="Fn1_source">1</a></sup> Potential services are not limited to individual end-users. By correlating daily activity patterns, communication providers can also offer services to communities of users with similar weekly habits, thus promoting and enhancing the use of the underlying communication infrastructure.</p><p class="Para">The services are not restricted to leisure-oriented activities. Understanding the physical situation of a user can also be used for early warning healthcare-related applications. Recognizing that an elderly person has fallen at his/her home and has not moved in the last 30Â s indicates a potential emergency situation for which a relative or a local emergency unit should be alerted. In civil protection scenarios, knowing the location and the state of readiness of the elements of an emergency response team could dramatically reduce dispatching time and thus the response lag.</p><p class="Para">A key enabler for these context-aware services that providers can now offer lies the ability of mobile devices to acquire, manage, process, and obtain useful information from raw sensor data. From these data, devices must be able to accurately discover the characteristics or <em class="EmphasisTypeItalic ">features</em> of the signal coming from a given sensor. Sensors do generate a high volume of raw data possibly contaminated with environment noise that needs to be filtered out. In addition, the device must generate a very low number of incorrectly recognized features to improve the accuracy of subsequent information processing stages where features are analyzed and organized into user context patterns.</p><div class="Para">The inclusion of sensors for context discovery in mobile devices is commonly organized as part of a software stack with a general architecture similar to the one depicted in Fig.Â <span class="InternalRef"><a href="#Fig1">1</a></span>. At the lowest levels of the stack, we have preprocessing phases where the device attempts to extract a set of basic features from the sensor signal. These features include specific short-term contexts or <em class="EmphasisTypeItalic ">states</em> such <em class="EmphasisTypeItalic ">absence of light, quick movement</em> or more sophisticated contexts such as <em class="EmphasisTypeItalic ">running</em>. It is based on these states that the next layerâthe base-level classifierâwill determine higher-level user contexts such as <em class="EmphasisTypeItalic ">activities</em> like jogging or exercising. Finally, a layer of application code and scripting will use the context history to infer daily or weekly activities or routines.<figure class="Figure" id="Fig1"><div class="MediaObject" id="MO10"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs00779-010-0293-9/MediaObjects/779_2010_293_Fig1_HTML.gif" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="https://media.springernature.com/lw785/springer-static/image/art%3A10.1007%2Fs00779-010-0293-9/MediaObjects/779_2010_293_Fig1_HTML.gif" alt="Fig.Â 1" /></a> </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig.Â 1</span> <p class="SimplePara">Layered architecture for context inference applications</p> </div></figcaption></figure> </div><p class="Para">In contrast with other sensors, which provide an instant value that can be used directly for context inference, the signal coming from an accelerometer may require the use of a fairly complex preprocessing stage in order to characterize the physical activity of the user within a certain time frame. Given the significance of this problem, a large number of techniques have been developed to address it. In this article, we survey the most representative domain approaches and techniques, including spectral analysis techniques such as Fast Fourier Transforms (FFT), statistics-based metrics and even string-matching approaches. These approaches vary widely in their context-recognition accuracy and often require specific input representations.</p><p class="Para">In Sect. <span class="InternalRef"><a href="#Sec2">2</a></span>, we survey a wide range of techniques used to recognize user activities; these techniques are organized into several broad domain approaches. Then in Sect. <span class="InternalRef"><a href="#Sec29">3</a></span>, we present the results on the application of these techniques to a set of experimental data to compare their benefits and computational cost. We conclude the article in Sect. <span class="InternalRef"><a href="#Sec38">4</a></span>.</p></div></section><section id="Sec2" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">2 </span>Preprocessing techniques: domains and approaches</h2><div class="content"><p class="Para">The need to extract key signal features that enable advanced processing algorithms to discover useful context information has led to the development of a wide range of algorithmic approaches. These approaches rely on converting or transforming the input signals to and from different domains of representation. In each domain there are specific methods to abstract raw signal data and to provide, in addition to an early classification, some form of data compression that makes it possible in many cases to apply higher-level algorithms for context recognition.</p><div class="Para">As depicted in Fig.Â <span class="InternalRef"><a href="#Fig2">2</a></span>, it is possible to classify the available sensor signal processing techniques is three broad domains, namely: the <em class="EmphasisTypeItalic ">time</em> domain, the <em class="EmphasisTypeItalic ">frequency</em> domain and what we call <em class="EmphasisTypeItalic ">discrete representation</em> domains. The following subsections describe the most representative techniques in each of these domains in order to compare their implementation complexity and accuracy in extracting signal features and identifying user activities.<figure class="Figure" id="Fig2"><div class="MediaObject" id="MO11"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs00779-010-0293-9/MediaObjects/779_2010_293_Fig2_HTML.gif" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="https://media.springernature.com/lw785/springer-static/image/art%3A10.1007%2Fs00779-010-0293-9/MediaObjects/779_2010_293_Fig2_HTML.gif" alt="Fig.Â 2" /></a> </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig.Â 2</span> <p class="SimplePara">Classification of techniques applied to sensor signals for feature extraction</p> </div></figcaption></figure> </div><section id="Sec3" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">2.1 </span>Time domain: mathematical and statistical techniques</h3><p class="Para">Simple mathematical and statistical metrics can be used to extract basic signal information from raw sensor data. In addition, these metrics are often used as preprocessing steps for metrics in other domains as a way to select key signal characteristics or features. These techniques are often used in practical activity recognition algorithms.</p><section id="Sec4" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.1.1 </span>Statistical metrics: mean, variance and standard deviation</h4><p class="Para">The mean over a window of data samples is a meaningful metric for almost every kind of sensor. This metric can be calculated with small computational cost [<span class="CitationRef"><a href="#CR48">48</a></span>] and be done on the fly with minimal memory requirements. The mean is usually applied in order to preprocess raw data by removing random spikes and noise (both mechanical and electrical) from sensor signals, smoothing the overall dataset.</p><p class="Para">There have been various early uses of the mean metric in activity recognition. Several researchers have used the mean to either directly or indirectly identify user posture (sitting, standing or lying) [<span class="CitationRef"><a href="#CR11">11</a></span>, <span class="CitationRef"><a href="#CR19">19</a></span>, <span class="CitationRef"><a href="#CR22">22</a></span>, <span class="CitationRef"><a href="#CR23">23</a></span>] and also to discriminate the <em class="EmphasisTypeItalic ">type</em> of activity as either dynamic or static [<span class="CitationRef"><a href="#CR60">60</a></span>]. Others have used the mean as input to classifiers like Neural Networks [<span class="CitationRef"><a href="#CR51">51</a></span>, <span class="CitationRef"><a href="#CR59">59</a></span>], Naive Bayes [<span class="CitationRef"><a href="#CR27">27</a></span>], Kohonen Self-Organizing Maps [<span class="CitationRef"><a href="#CR29">29</a></span>], Decision Trees [<span class="CitationRef"><a href="#CR5">5</a></span>], and even Fuzzy Inference [<span class="CitationRef"><a href="#CR20">20</a></span>]. Other applications of the mean value include, for example, axial calibration by finding the average value for all the different orientations [<span class="CitationRef"><a href="#CR7">7</a></span>] and the recognition of complex human gesture using Hidden Markov Models [<span class="CitationRef"><a href="#CR8">8</a></span>].</p><p class="Para">Another important statistical metric is the variance (Ï<sup>2</sup>) defined as the average of the squared differences from the mean. The standard deviation (Ï) is the square-root of the variance and represents both the variability of a data set and a probability distribution. The standard deviation can give an indication of the stability of a signal. The measure is less useful if it is known that the signal can include spurious values, as even a single value can distort the result.</p><p class="Para">These two statistical metrics are often used as a signal feature in many activity recognition approaches where they have been used as an input to a classifier or to threshold-based algorithms [<span class="CitationRef"><a href="#CR12">12</a></span>, <span class="CitationRef"><a href="#CR15">15</a></span>, <span class="CitationRef"><a href="#CR30">30</a></span>].</p><p class="Para">In other approaches [<span class="CitationRef"><a href="#CR22">22</a></span>, <span class="CitationRef"><a href="#CR23">23</a></span>], the variance and standard deviation were used to infer user movement or have been used as the base metric for classifiers like the Naive Bayes [<span class="CitationRef"><a href="#CR27">27</a></span>], Dynamic Bayesian Networks [<span class="CitationRef"><a href="#CR61">61</a></span>], Neural Networks [<span class="CitationRef"><a href="#CR59">59</a></span>] and by [<span class="CitationRef"><a href="#CR41">41</a></span>] to identify the mode of transport. The variance has also been used in [<span class="CitationRef"><a href="#CR58">58</a></span>] in a combination with other metrics such as mean and maximum.</p></section><section id="Sec5" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.1.2 </span>Envelope metrics: median, maximum, minimum and range</h4><p class="Para">The median is the number that separates the higher half of data samples from the lower half. The value provided by the median is typically used to replace missing values from a sequence of discrete values (see e.g., [<span class="CitationRef"><a href="#CR63">63</a></span>]).</p><p class="Para">Despite their simplicity, these envelope metrics still offer some value in activity recognition (e.g., [<span class="CitationRef"><a href="#CR2">2</a></span>]) or as an input to Neural Networks for identifying different inclination angles while walking, as well as in to distinguish between types of postures with threshold-based techniques [<span class="CitationRef"><a href="#CR3">3</a></span>].</p><p class="Para">The range (the difference between maximum and minimum sample values) was used in [<span class="CitationRef"><a href="#CR11">11</a></span>] together with other indicators to discriminate between walking and running. Application of the maximum and minimum values in accelerometer-based systems was explored to detect steps with the Twiddler keyboard [<span class="CitationRef"><a href="#CR4">4</a></span>], to detect gestures as mnemonical body shortcuts [<span class="CitationRef"><a href="#CR13">13</a></span>], and in activity recognition as an input to a Neural Network classifier [<span class="CitationRef"><a href="#CR59">59</a></span>].</p></section><section id="Sec6" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.1.3 </span>Root mean square (RMS) metric</h4><div class="Para">The root mean square (RMS) of a signal <em class="EmphasisTypeItalic ">x</em> <sub> <em class="EmphasisTypeItalic ">i</em> </sub> that represents a sequence of <em class="EmphasisTypeItalic ">n</em> discrete values {<em class="EmphasisTypeItalic ">x</em> <sub>1</sub>,Â <em class="EmphasisTypeItalic ">x</em> <sub>2</sub>,â¦,Â <em class="EmphasisTypeItalic ">x</em> <sub> <em class="EmphasisTypeItalic ">n</em> </sub>} is obtained using Eq. (<span class="InternalRef"><a href="#Equ1">1</a></span>) and can be associated with meaningful context information.
<div id="Equ1" class="Equation EquationMathjax"><div class="EquationContent">$$ x_{\rm RMS}=\sqrt{\frac{x_{1}^{2}+x_{2}^{2}+\cdots+x_{n}^{2}} {n}} $$</div> <div class="EquationNumber">(1)</div></div> </div><p class="Para">The RMS has been used to classify wavelet results by distinguishing walking patterns [<span class="CitationRef"><a href="#CR53">53</a></span>] and is present in works of activity recognition like [<span class="CitationRef"><a href="#CR42">42</a></span>] as an input to a classifier such as a Neural Network. It was also used as input to a multi-layer neural network in [<span class="CitationRef"><a href="#CR8">8</a></span>] for the recognition of a set of gestures and integration with video records.</p></section><section id="Sec7" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.1.4 </span>Position and velocity using numeric integration</h4><p class="Para">The integration metric measures the signal area under the data curve and is commonly applied to accelerometer signals to obtain estimates of speed and distance [<span class="CitationRef"><a href="#CR40">40</a></span>].</p><p class="Para">Several approaches have explored this integration technique. In gesture recognition (e.g., [<span class="CitationRef"><a href="#CR13">13</a></span>]), researchers have used a double integration technique to compute the distance covered by a gesture. Others have used this technique to determined velocity values and thus identify gestures using Nintendo Wiimote and a Neural Network classifier [<span class="CitationRef"><a href="#CR63">63</a></span>].</p><p class="Para">Using the integral of the RMS signal and a simple threshold technique, researchers have been able to distinguish between higher and lower states of activity intensity [<span class="CitationRef"><a href="#CR15">15</a></span>]. In other approaches, Lee etÂ al. [<span class="CitationRef"><a href="#CR30">30</a></span>] used integration to compute the angular velocity of data supplied by a gyroscope.</p></section><section id="Sec8" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.1.5 </span>Signal correlation and correlation coefficient</h4><p class="Para">Signal correlation is used to measure the strength and direction of a linear relationship between two signals. In activity recognition, correlation is especially useful in differentiating between activities that involve translation in a single dimension [<span class="CitationRef"><a href="#CR43">43</a></span>].</p><div class="Para">In order to calculate the degree of correlation, it is necessary to calculate the correlation coefficients between the signals for the various axes. These coefficients can be obtained by several statistical and geometrical formulas, depending on whether the situation involves simple statistical values or mathematical vectors. The most commonly used is Pearsonâs product-moment coefficient (Ï<sub><em class="EmphasisTypeItalic ">x</em>,<em class="EmphasisTypeItalic ">y</em></sub>) [<span class="CitationRef"><a href="#CR45">45</a></span>] also known as the sample correlation coefficient, calculated as the ratio of the covariance of the signals along the <em class="EmphasisTypeItalic ">x</em>-axis and <em class="EmphasisTypeItalic ">y</em>-axis to the product of their standard deviations:
<div id="Equ2" class="Equation EquationMathjax"><div class="EquationContent">$$ \rho_{x,y}=\frac{{cov}(x,y)}{\sigma_{x}\sigma_{y}} $$</div> <div class="EquationNumber">(2)</div></div> </div><p class="Para">The sample correlation coefficient was applied in [<span class="CitationRef"><a href="#CR43">43</a></span>] in order to determine which are the best classifiers (or combination of them) for recognizing activities, and which among several features/attributes are the most important. Other researchers (e.g., [<span class="CitationRef"><a href="#CR57">57</a></span>]) also used this coefficient with accelerometer data to correlate the various axes in order to detect, with the aid of a decision tree, the surface under a robot as it walks.</p><p class="Para">Another way to calculate the correlation coefficient is by geometric interpretation. For normalized or centered data sets, with a mean of zero, the correlation coefficient can also be obtained as the cosine of the angle between two vectors of samples. To determine the correlation coefficient using the angle, the product of vectors representing each axis of the sensor has been used. In [<span class="CitationRef"><a href="#CR28">28</a></span>], the authors applied the normalized cross product between each axis of an accelerometer, together with mean, energy and entropy, to identify a set of daily activities. Other researchers [<span class="CitationRef"><a href="#CR5">5</a></span>, <span class="CitationRef"><a href="#CR16">16</a></span>, <span class="CitationRef"><a href="#CR18">18</a></span>] used the dot product divided by the window length as the correlation feature, computed between two different acceleration axes of a hoarder board.</p></section><section id="Sec9" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.1.6 </span>Cross-correlation</h4><div class="Para">The cross-correlation is a measure of the similarity between two waveforms and is commonly used to search for a known pattern in a long signal. The cross-correlation coefficients are calculated by computing a dot product between the signals, normalized over the window size of <em class="EmphasisTypeItalic ">n</em> samples as denoted by Eq. (<span class="InternalRef"><a href="#Equ3">3</a></span>). The various coefficients are obtained by computing the correlation for the âtime shiftedâ versions of one signal with respect to the other.
<div id="Equ3" class="Equation EquationMathjax"><div class="EquationContent">$$ {Cross\, Correlation_{(x,y)}=\max_{d=1}^{n-1}\left(\frac{1}{n}\sum_{i=1}^{n} x_{i}\cdot y_{i-d}\right)} $$</div> <div class="EquationNumber">(3)</div></div> </div><p class="Para">The typical implementation of this metric computes the cross-correlation coefficients for the pairs of signals corresponding to the three axes in a pairwise fashion (i.e., (<em class="EmphasisTypeItalic ">x</em>,<em class="EmphasisTypeItalic ">y</em>), (<em class="EmphasisTypeItalic ">x</em>,<em class="EmphasisTypeItalic ">z</em>) and (<em class="EmphasisTypeItalic ">y</em>,<em class="EmphasisTypeItalic ">z</em>)). It then selects the pair of signals that exhibits the largest coefficients to distinguish between dynamic activities, as described in [<span class="CitationRef"><a href="#CR60">60</a></span>].</p></section></section><section id="Sec10" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">2.2 </span>Other time-domain metrics</h3><p class="Para">There are several other time-domain characteristics that can be obtained directly from raw accelerometer data and that are popular since they require less computing power than signal characteristics calculated in other domains.</p><section id="Sec11" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.2.1 </span>Sample differences</h4><p class="Para">Computing the difference (delta value) between signals in a pairwise arrangement of samples allows a basic comparison between the intensity of user activity. In general, every activity will be noticeable in one or more of the accelerometer axis, so different activities can in principle be distinguished by comparing the signal strength in all three axis.</p><p class="Para">This approach has been used in [<span class="CitationRef"><a href="#CR55">55</a></span>] to make a crude detection between classes of activities by inspecting of the maximum values (or peaks) of the differences between all axes to determine the type of movement.</p></section><section id="Sec12" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.2.2 </span>Zero-crossings</h4><p class="Para">Zero-crossing can be defined as the points where a signal passes through a specific value corresponding to half of the signal range. The delimiter value can be either the mean value of the sensor range or an extracted mean value. The number of times the signal crosses the reference value is the number of zero-crossings.</p><p class="Para">This metric has been used to make early recognition of stepping movements [<span class="CitationRef"><a href="#CR30">30</a></span>] and for the detection of the appropriate timing for the application of other techniques (e.g., [<span class="CitationRef"><a href="#CR11">11</a></span>]) to distinguish walking from running. Zero-crossings rate is the rate of zero-crossings along a signal and is commonly applied to audio signals to identify the surrounding environment [<span class="CitationRef"><a href="#CR10">10</a></span>] or the type of sound such as music, speech and noise [<span class="CitationRef"><a href="#CR49">49</a></span>]. Also, in gesture recognition the zero-crossings have been used together with other features and with Hidden Markov Models to detect complex human gestures [<span class="CitationRef"><a href="#CR8">8</a></span>].</p></section><section id="Sec13" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.2.3 </span>Angle and angular velocity</h4><p class="Para">The use of angular position and velocity together with the use of other sensor data and integration techniques allows a basic determination of the user orientation. The angle between the accelerometer axis and the gravity pull can be determined from the mean of the accelerometer signal in all three axis [<span class="CitationRef"><a href="#CR21">21</a></span>, <span class="CitationRef"><a href="#CR60">60</a></span>].</p><p class="Para">This metric has been used as described in [<span class="CitationRef"><a href="#CR30">30</a></span>] to identify cyclic behavior using a simple fuzzy-logic reasoning method. Angle variation has been used in fall detection [<span class="CitationRef"><a href="#CR7">7</a></span>, <span class="CitationRef"><a href="#CR9">9</a></span>] to check sudden variation and final orientation of the user. To detect the location of a device equipped with an accelerometer from a predetermined set of locations, it is possible to use the angle with gravity and its variation along time [<span class="CitationRef"><a href="#CR23">23</a></span>].</p></section><section id="Sec14" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.2.4 </span>Signal magnitude area</h4><div class="Para">In [<span class="CitationRef"><a href="#CR6">6</a></span>], the authors describe an approach that uses the sum of the area encompassed by the magnitude of each of the three-axis accelerometer signals to compute the <em class="EmphasisTypeItalic ">energy expenditure</em> in daily activities. This activity predictor is referred to as the Signal Magnitude Area (SMA) [<span class="CitationRef"><a href="#CR37">37</a></span>] as depicted in Eq.Â <span class="InternalRef"><a href="#Equ4">4</a></span> where <em class="EmphasisTypeItalic ">x</em>(<em class="EmphasisTypeItalic ">t</em>), <em class="EmphasisTypeItalic ">y</em>(<em class="EmphasisTypeItalic ">t</em>) and <em class="EmphasisTypeItalic ">z</em>(<em class="EmphasisTypeItalic ">t</em>) are the acceleration signals from each axis with respect to time <em class="EmphasisTypeItalic ">t</em>.
<div id="Equ4" class="Equation EquationMathjax"><div class="EquationContent">$$ {\rm SMA}=\frac{1}{t}\left(\int\limits_{0}^{t}|x(t)|{\rm d}t+\int\limits_{0}^{t}|y(t)|{\rm d}t+\int\limits_{0}^{t}|z(t)|{\rm d}t\right) $$</div> <div class="EquationNumber">(4)</div></div> </div><p class="Para">The SMA metric can be used to distinguish between a resting state and user activity [<span class="CitationRef"><a href="#CR38">38</a></span>] in a classification framework for the recognition of basic daily movements. It is also possible to use SMA as the basis for identifying periods of user activity [<span class="CitationRef"><a href="#CR21">21</a></span>].</p></section><section id="Sec15" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.2.5 </span>Signal vector magnitude</h4><div class="Para">Signal Vector Magnitude (SVM) [<span class="CitationRef"><a href="#CR21">21</a></span>] and Differential Signal Vector Magnitude (DSVM) [<span class="CitationRef"><a href="#CR19">19</a></span>] metrics are similar to the norm as defined below:
<div id="Equ5" class="Equation EquationMathjax"><div class="EquationContent">$$ {\rm SVM}=\frac{1}{n} \sum_{i=1}^{n}\sqrt{x_{i}^{2}+y_{i}^{2}+z_{i}^{2}} \quad {\rm DSVM}=\frac{1}{t} \left(\int\limits_{0}^{t}\left(|\sum {\rm SVM}^{\prime}|\right){\rm d}t\right) $$</div> <div class="EquationNumber">(5)</div></div> </div><p class="Para">The SVM metric was used by [<span class="CitationRef"><a href="#CR21">21</a></span>] to identify possible falls and to monitor and classify behavior patterns in cattle using an accelerometer attached to the hind legs [<span class="CitationRef"><a href="#CR44">44</a></span>]. DSVM was developed to facilitate the classification of dynamic daily activities, including falls, using a single metric and several thresholds operations.</p></section></section><section id="Sec16" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">2.3 </span>Frequency domain</h3><p class="Para">Frequency-domain techniques have been extensively used to capture the repetitive nature of a sensor signal. This repetition often correlates to the periodic nature of a specific activity such as walking or running. A commonly used signal transformation technique is the Fourier transform, which allows one to represent in the frequency domain (or spectrum) important characteristics of a time-based signal such as its average (or DC component) and dominant frequency components. In this spectral representation, the main periods or repetition intervals of the signal are represented by non-zero values or coefficients at the corresponding frequency axis value. For instance, a time signal with periodic patterns centered around the 0.5-s repetition interval will exhibit a noticeable Fourier coefficient centered around the 2Â Hz frequency axis. This frequency analysis is commonly computed for a time signal of a specific length or window using the discrete Fourier transform with algorithms such as the Fast Fourier Transform (FFT) and the Fast Time Frequency Transform (FTFT) [<span class="CitationRef"><a href="#CR36">36</a></span>]. In addition to the FFT and its spectral representation, other frequency-based representation has been used. For example, the Wavelet Haar transforms [<span class="CitationRef"><a href="#CR34">34</a></span>] represent a time-domain signal as a decomposition of a set of weighted orthonormal vector basis or coefficients. These transforms, although less common, provide computational advantages over the more established FFT computation.</p><p class="Para">The following subsections highlight the common uses of frequency-domain analysis for the recognition of user activity from accelerometer data.</p><section id="Sec17" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.3.1 </span>DC component</h4><p class="Para">The DC component is the first coefficient in the spectral representation of a signal and its value is often much larger than the remaining spectral coefficients. As described earlier, the mean is used as signal characteristic in several activity recognition approaches along with correlation, energy and entropy [<span class="CitationRef"><a href="#CR5">5</a></span>, <span class="CitationRef"><a href="#CR16">16</a></span>, <span class="CitationRef"><a href="#CR18">18</a></span>, <span class="CitationRef"><a href="#CR28">28</a></span>].</p></section><section id="Sec18" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.3.2 </span>Spectral energy</h4><p class="Para">The energy of the signal can be computed as the squared sum of its spectral coefficients normalized by the length of the sample window. The energy metric has been used [<span class="CitationRef"><a href="#CR41">41</a></span>] to identify the mode of transport of a user with a single accelerometer, respectively, walking, cycling, running and driving. In other contexts, researchers used a microphone sensor to obtain an audio context and thus identify when a user would be on the street, engaged in a conversation, or indoors in a loud (e.g., restaurant) or in a quiet place (e.g., lecture) [<span class="CitationRef"><a href="#CR27">27</a></span>].</p></section><section id="Sec19" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.3.3 </span>Information entropy</h4><p class="Para">The entropy metric can be computed using the normalized information entropy of the discrete FFT coefficient magnitudes excluding the DC component [<span class="CitationRef"><a href="#CR16">16</a></span>]. Entropy helps to differentiate between signals that have similar energy values but correspond to different activity patterns. Together with the mean, energy and correlation, entropy has been used in several activity recognition approaches. For example, Bao etÂ al. [<span class="CitationRef"><a href="#CR5">5</a></span>] have used frequency-domain entropy to distinguish between activities with similar energy levels, as is the case of cycling and jogging.</p></section><section id="Sec20" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.3.4 </span>Spectral analysis of key coefficients</h4><p class="Para">Several authors have used the summation of a set of spectral coefficients as a key metric for the recognition of specific activities. For example, the coefficients from 0.5Â Hz to 3Â Hz can be used as the key discriminating coefficients for the running and walking activities [<span class="CitationRef"><a href="#CR47">47</a></span>, <span class="CitationRef"><a href="#CR62">62</a></span>].</p><p class="Para">In [<span class="CitationRef"><a href="#CR37">37</a></span>], the author developed an algorithm to determine the average step rate from the signal spectrum. The algorithm looks for a frequency peak within the 0.7â3Â Hz range. The magnitude of the largest signal peak is compared against the baseline noise value. If the signal-to-noise ratio (SNR) is greater than a fixed threshold value then the frequency at which this peak occurs is identified as the step rate. Other authors have used the frequency value corresponding to the maximal spectral coefficient to determine whether a person is either walking or running, and if running at what pace [<span class="CitationRef"><a href="#CR22">22</a></span>, <span class="CitationRef"><a href="#CR23">23</a></span>, <span class="CitationRef"><a href="#CR24">24</a></span>].</p></section><section id="Sec21" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.3.5 </span>Wavelet analysis</h4><p class="Para">The Wavelet transform can be used to examine the time-frequency characteristics of a signal as it can be computed more efficiently than the Fourier transform [<span class="CitationRef"><a href="#CR35">35</a></span>]. For the spectral representation, the Wavelet transform makes use of a set of orthonormal basis typically chosen from a family of possible base generating functions. Among the many possible transforms we have opted for the simpler Haar transforms (see e.g., [<span class="CitationRef"><a href="#CR34">34</a></span>]), and in this family of transforms we used the transform of order 2, i.e., <em class="EmphasisTypeItalic ">H</em> <sub>2</sub>. This particular transform only requires additions and subtractions for the computation of the spectral coefficients and can be performed <em class="EmphasisTypeItalic ">in place</em>, thus not requiring any additional storage. Once the spectral coefficients are computed, a simple approach is to add all the coefficients thus generating a single metric value.</p><p class="Para">Because the wavelet transform can capture sudden changes in signals like the ones measured by an accelerometer, it is often chosen by several activity recognition approaches. However, the direct use of wavelets in the detection of user physical activities raises several issues as there exist many wavelet transforms and of various kinds. In addition, the resulting representation (the wavelet coefficients) do not capture any quantity with physical meaning. As a result, wavelets are commonly used in conjunction with other higher-level techniques (e.g., [<span class="CitationRef"><a href="#CR1">1</a></span>]).</p><p class="Para">In [<span class="CitationRef"><a href="#CR52">52</a></span>, <span class="CitationRef"><a href="#CR53">53</a></span>, <span class="CitationRef"><a href="#CR54">54</a></span>], the authors use the discrete wavelet transform to classify the acceleration signal for horizontal level and stairway walking. Other authors [<span class="CitationRef"><a href="#CR1">1</a></span>] have used the Daubechies wavelets over the preprocessed signals with the SVM transformation. They then feed the data to a Hidden Markov Model to detect human activity with special attention to falls of elderly people.</p></section></section><section id="Sec22" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">2.4 </span>Symbolic strings domain</h3><p class="Para">There has been a recent interest in transforming accelerometer and other sensor signals into strings of discrete symbols. A key aspect in this transformation has been the discretization process, and while there is a potential for information loss, a limited symbol alphabet can lead to a substantial compression in the representation of a signal. Typically, the sequence of <em class="EmphasisTypeItalic ">n</em> input samples (possibly already normalized as in the case of a 3-axis accelerometer signal) is split into windows of <em class="EmphasisTypeItalic ">w</em> consecutive samples. An average value is computed for each of these windows followed by a discretization over a fixed size alphabet <em class="EmphasisTypeItalic ">a</em>.</p><p class="Para">A simple discretization process uses a domain-value function that defines the interval of values that correspond to a given symbol. A recently developed technique called symbolic aggregate approximation (SAX) uses a piecewise aggregate approximation (PAA) [<span class="CitationRef"><a href="#CR31">31</a></span>], which relies on a gaussian equiprobable distribution function to map range values into string symbols.</p><p class="Para">Once signals have been mapped to strings, exact or approximate matching and edit distances are key techniques used to evaluate string similarity and thus either find known patterns or classify the user activity.</p><section id="Sec23" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.4.1 </span>Euclidian-related distances</h4><div class="Para">For two strings <em class="EmphasisTypeItalic ">S</em> and <em class="EmphasisTypeItalic ">T</em> of length <em class="EmphasisTypeItalic ">n</em> we define the Euclidian distance as
<div id="Equ6" class="Equation EquationMathjax"><div class="EquationContent">$$ {EuclidianDist}(S,T)=\sqrt{\sum_{i=1}^{n} (|s_i - t_i|)^2} $$</div> <div class="EquationNumber">(6)</div></div>where the distance between symbols is defined by the corresponding numeric distance between the signal values that correspond to each symbol in the string representation.</div><div class="Para">A related metric often used for time-series classification that is a lower bound on the Euclidian distance is the minimum distance (<em class="EmphasisTypeItalic ">MinDist</em>) metric [<span class="CitationRef"><a href="#CR31">31</a></span>] defined as:
<div id="Equ7" class="Equation EquationMathjax"><div class="EquationContent">$$ {MinDist}(S,T)=\sqrt{\frac{n}{w}}\cdot\sqrt{\sum_{i=1}^{n} {dist}(s_i - t_i)^2} $$</div> <div class="EquationNumber">(7)</div></div>where <em class="EmphasisTypeItalic ">n</em> is the length of the two strings, and the symbols in both strings have been quantized using a Gaussian distribution. The <em class="EmphasisTypeItalic ">dist</em> function is a symmetric distance matrix based on the cut-off points of the same Gaussian. The fraction <span class="InlineEquation" id="IEq1">\(\sqrt{\frac{n}{w}}\)</span> is a normalization of the compression ratio achieved by the SAX transformation.</div><p class="Para">These distance metrics, albeit simple, allow for the quick discrimination of signals and thus for fast evaluation of similarity between strings.</p></section><section id="Sec24" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.4.2 </span>Levenshtein edit distance</h4><p class="Para">Having a representation based on strings of symbols allows the use of a plethora of string-approximation algorithms in order to determine which of a set of representative samples is âclosestâ to a given input sample.</p><div class="Para">One of such metrics is the Levenshtein edit distance. For two strings <em class="EmphasisTypeItalic ">S</em> and <em class="EmphasisTypeItalic ">T</em>, this metric determines the minimum number of symbol insertions, deletion and substitutions needed to transform one string into the other. A common implementation uses dynamic-programming techniques with the recurrent expression [<span class="CitationRef"><a href="#CR14">14</a></span>]:<div id="Equ8" class="Equation EquationMathjax"><div class="EquationContent">$$ d(i,j) = {min}\left\{d(i-1,j) + {insert}, d(i,j-1) + {insert}, d(i-1,j-1) + {subs}(i, j)\right\} $$</div> <div class="EquationNumber">(8)</div></div>where <em class="EmphasisTypeItalic ">m</em> and <em class="EmphasisTypeItalic ">n</em> are the length of the two strings and <em class="EmphasisTypeItalic ">d</em> is a <em class="EmphasisTypeItalic ">m</em> ÃÂ <em class="EmphasisTypeItalic ">n</em> table whose first row and column are initialized with the costs of creating each of the input strings, i.e., by inserting all their symbols. The minimal cost of converting <em class="EmphasisTypeItalic ">S</em> into <em class="EmphasisTypeItalic ">T</em> is determined by the value of the table position <em class="EmphasisTypeItalic ">d</em>(<em class="EmphasisTypeItalic ">n</em>,Â <em class="EmphasisTypeItalic ">m</em>).</div><p class="Para">In [<span class="CitationRef"><a href="#CR56">56</a></span>], the authors used this distance metric to identify a number of gestures with reported 83% accuracy using a particular training methodology. According to the trajectory of the movement, one of seven symbols is attributed to the signal acquired from a distributed set of inertial sensor modules mounted on the lower arms, the upper arms and the torso of the body. During training, some template samples of the gestures were collected and through a string-matching method based on the computation of weighted edit distances, each gesture is identified.</p></section><section id="Sec25" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.4.3 </span>Dynamic time warping (DTW)</h4><p class="Para">Dynamic time warping (DTW) [<span class="CitationRef"><a href="#CR46">46</a></span>] is a metric for measuring similarity between two sequences that may vary in length and can thus correspond to different time basis. DTW is used in automatic speech recognition based on a temporal alignment of the input signal with template models. It can capture similarities of strings with distinct sampling period, and thus speeds, but has a relatively high computational cost. The goal is to find a mapping <em class="EmphasisTypeItalic ">W</em>, where in some cases an element of one string can map to a sequence of consecutive elements in the other string.</p><div class="Para">The DTW algorithm implementation builds a matrix of mapping costs, along which the mapping between the two strings is laid out as a path between the two opposite corners. Of the many possible such paths, DTW finds the mapping <em class="EmphasisTypeItalic ">W</em> that minimizes:<div id="Equ9" class="Equation EquationMathjax"><div class="EquationContent">$$ {\rm DTW}(S,T)= {min}\left\{\frac{1}{K}\cdot\sqrt{\sum_{k=1}^{K} w_k}\right\} $$</div> <div class="EquationNumber">(9)</div></div>where the cost of the path through the cost matrix is found using a dynamic-programming approach as the example of the metric in the previous subsection.</div><p class="Para">DTW has been applied to find similarity metrics between signals [<span class="CitationRef"><a href="#CR26">26</a></span>] in particular in the context of gesture recognition, given the potentially more limited number of required samples [<span class="CitationRef"><a href="#CR33">33</a></span>]. More recently, researchers have also developed the <em class="EmphasisTypeItalic ">derivative</em> DTW (DDTW) [<span class="CitationRef"><a href="#CR25">25</a></span>] and used it to detect activities such as walking, going up and down flights of stairs [<span class="CitationRef"><a href="#CR39">39</a></span>].</p></section></section><section id="Sec26" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">2.5 </span>Analysis of suitability of implementation</h3><p class="Para">The various techniques described in Sect. <span class="InternalRef"><a href="#Sec3">2.1</a></span> through Sect. <span class="InternalRef"><a href="#Sec22">2.4</a></span> have different computational costs and storage requirements making them more or less suitable for implementation on mobile devices such as smartphones with limited storage or computational resources. We now discuss these implementation costs first on an quantitative basis using abstract complexity measures, and then qualitatively by using these measures to provide an assessment of the suitability of the use of each metric in mobile devices.</p><section id="Sec27" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.5.1 </span>Quantitative analysis: complexity of implementation</h4><div class="Para">Rather than using device- or processor-dependent metrics that could be masked by platform-specific features or even by the capabilities of the corresponding compilers<sup><a href="#Fn2" id="Fn2_source">2</a></sup>, we opted for an abstract complexity measure that is tied to the number of operations in the computation of each metric. As such, we divide the costs into several components, where each component becomes a function of the number of samples <em class="EmphasisTypeItalic ">n</em> in the input data, possibly separated for each of the three accelerometer axes. In the computational cost, we have included the number of additions, multiplications and other arithmetic and logic operations. For simplicity, all operations are assumed to use floating-point (32-bit single precision) representations. In some metrics, there is the need for an initial normalization step in order to transform three integer input vectors (one for each of the three accelerometer axis) into a single floating-point vector. So that this step does not mask the inherent complexity of each metric, we present the complexity results for each metric excluding the cost of this preliminary step. Some metrics, however, do not require this initialization step and are labeled with an asterisk (*) in TableÂ <span class="InternalRef"><a href="#Tab1">1</a></span>. For completeness, we have also included the number of comparison operations, as invariably their implementation will include an arithmetic operation.<div id="Tab1" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">TableÂ 1</span> <p class="SimplePara">Summary of classification of time-domain metrics regarding computational costs, where <em class="EmphasisTypeItalic ">n</em> is the number of input samples</p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1" /><col class="tcol2" /><col class="tcol3" /><col class="tcol4" /><col class="tcol5" /><col class="tcol6" /></colgroup><thead><tr><th rowspan="2"> <p class="SimplePara">Time-domain metric</p> </th><th colspan="5"> <p class="SimplePara">Computational complexity</p> </th></tr><tr><th> <p class="SimplePara">Add</p> </th><th> <p class="SimplePara">Mult</p> </th><th> <p class="SimplePara">Div</p> </th><th> <p class="SimplePara">Sqrt</p> </th><th> <p class="SimplePara">Comparisons</p> </th></tr></thead><tbody><tr><td> <p class="SimplePara">Normalization</p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Mean</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">1</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Std. deviation</p> </td><td> <p class="SimplePara">2<em class="EmphasisTypeItalic ">n</em>Â +Â 1</p> </td><td> <p class="SimplePara"><em class="EmphasisTypeItalic ">n</em>Â +Â 1</p> </td><td> <p class="SimplePara">2</p> </td><td> <p class="SimplePara">1</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Median</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara"><em class="EmphasisTypeItalic ">n</em>(<em class="EmphasisTypeItalic ">n</em>Â +Â 1)/2</p> </td></tr><tr><td> <p class="SimplePara">Range</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">2<em class="EmphasisTypeItalic ">n</em> </p> </td></tr><tr><td> <p class="SimplePara">Maximum</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td></tr><tr><td> <p class="SimplePara">Minimum</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td></tr><tr><td> <p class="SimplePara">RMS</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">1</p> </td><td> <p class="SimplePara">1</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Integration</p> </td><td> <p class="SimplePara">2<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Correlation*</p> </td><td> <p class="SimplePara">4<em class="EmphasisTypeItalic ">n</em>Â +Â 4</p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em>Â +Â 1</p> </td><td> <p class="SimplePara">7</p> </td><td> <p class="SimplePara">3</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Cross-correlation*</p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em>(<em class="EmphasisTypeItalic ">n</em>Â âÂ 1)</p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em>(<em class="EmphasisTypeItalic ">n</em>Â âÂ 2)</p> </td><td> <p class="SimplePara">3(<em class="EmphasisTypeItalic ">n</em>Â âÂ 1)</p> </td><td> <p class="SimplePara">3</p> </td><td> <p class="SimplePara">3(<em class="EmphasisTypeItalic ">n</em>Â âÂ 1)</p> </td></tr><tr><td> <p class="SimplePara">Differences</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">1</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">5<em class="EmphasisTypeItalic ">n</em> </p> </td></tr><tr><td> <p class="SimplePara">Zero-crossings</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">1</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">2<em class="EmphasisTypeItalic ">n</em> </p> </td></tr><tr><td> <p class="SimplePara">SMA*</p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em>Â +Â 6</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em>Â âÂ 2</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">SVM*</p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">6<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">1</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">DSVM*</p> </td><td> <p class="SimplePara">4<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">6<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara"><em class="EmphasisTypeItalic ">n</em>Â +Â 1</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td></tr></tbody></table></div></div> </div><div class="Para">In separate tables, such as TableÂ <span class="InternalRef"><a href="#Tab2">2</a></span>, we have included the memory requirements to hold temporary variables in addition to the space required to hold the input signal data. These tables also include the number of memory <em class="EmphasisTypeItalic ">read</em> and <em class="EmphasisTypeItalic ">write</em> operations to the storage, excluding operations on scalar variables, which tend to be absorbed in register instructions that implement arithmetic operations. Notice that the vast majority of the metrics have 0 values for the <em class="EmphasisTypeItalic ">read</em> and <em class="EmphasisTypeItalic ">write</em> counts as the computation can be done <em class="EmphasisTypeItalic ">on-the-fly</em> since the normalization step is generating the values for each sample.<div id="Tab2" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">TableÂ 2</span> <p class="SimplePara">Summary of classification of time-domain metrics regarding storage costs and memory operations, where <em class="EmphasisTypeItalic ">n</em> is the number of input samples</p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1" /><col class="tcol2" /><col class="tcol3" /><col class="tcol4" /></colgroup><thead><tr><th rowspan="2"> <p class="SimplePara">Time-domain metric</p> </th><th rowspan="2"> <p class="SimplePara">Storage</p> </th><th colspan="2"> <p class="SimplePara">Memory</p> </th></tr><tr><th> <p class="SimplePara">Read</p> </th><th> <p class="SimplePara">Write</p> </th></tr></thead><tbody><tr><td> <p class="SimplePara">Normalization</p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Mean</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Std. deviation</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Median</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara"><em class="EmphasisTypeItalic ">n</em>(<em class="EmphasisTypeItalic ">n</em>Â +Â 1)/2</p> </td><td> <p class="SimplePara"><em class="EmphasisTypeItalic ">n</em>(<em class="EmphasisTypeItalic ">n</em>Â +Â 1)/2</p> </td></tr><tr><td> <p class="SimplePara">Range</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Maximum</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Minimum</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">RMS</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Integration</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Correlation*</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">2<em class="EmphasisTypeItalic ">n</em> </p> </td></tr><tr><td> <p class="SimplePara">Cross-correlation*</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em> <sup>2</sup> </p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Differences</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Zero-crossings</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">SMA*</p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">SVM*</p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">DSVM*</p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td></tr></tbody></table></div></div> </div><p class="Para">Overall, these complexity results show that time-domain metrics are dominated by the cost of normalization and none of them involve complex arithmetic operations such as trigonometric or logarithmic functions. Some of the simple metrics exhibit a very small number of multiplications, in some extreme cases (e.g., Differences) only additions and comparisons are required. In general, any of these metrics can be a good candidate for implementation on mobile devices where resources (computing, energy and storage) are at a premium.</p><p class="Para">As with the computational complexity costs, the storage and memory operations are dominated by the normalization step. In general, all metrics have memory requirements comparable to the normalization with the exception of the <em class="EmphasisTypeItalic ">median</em> metric. The <em class="EmphasisTypeItalic ">median</em> metric uses a <em class="EmphasisTypeItalic ">bubble sort</em> algorithm implementation to find the median value and thus the <em class="EmphasisTypeItalic ">O</em>(<em class="EmphasisTypeItalic ">n</em> <sup>2</sup>) complexity in terms of comparisons and read/write operations. For long signals with a large number of input samples, asymptotically more efficient algorithms such as <em class="EmphasisTypeItalic ">merge sort</em> with <em class="EmphasisTypeItalic ">O</em>(<em class="EmphasisTypeItalic ">n</em> log<em class="EmphasisTypeItalic ">n</em>) could reduce the number of such operations at the expense of a more complicated control-flow implementation of the sorting algorithm.</p><div class="Para">TablesÂ <span class="InternalRef"><a href="#Tab3">3</a></span> and <span class="InternalRef"><a href="#Tab4">4</a></span> provide a similar account on the time and space complexity of frequency-domain techniques. This analysis shows that overall these frequency-domain metrics are computationally more expensive than the time-domain metrics, not being necessarily dominated by the normalization phase. When computing the FFT of the normalized input signal, the implementation uses a precomputed table with <em class="EmphasisTypeItalic ">sine</em> and <em class="EmphasisTypeItalic ">cosine</em> values for various discrete points between positions 0 and <em class="EmphasisTypeItalic ">n</em>Â âÂ 1 thus avoiding expensive trigonometric computations. For the <em class="EmphasisTypeItalic ">entropy</em> metric, however, the use of the logarithmic operator is required. Regarding the storage requirements as depicted in TableÂ <span class="InternalRef"><a href="#Tab4">4</a></span>, these frequency-domain metrics also require more space than the time-domain metrics but still only as a linear function with respect to the number of input samples.<div id="Tab3" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">TableÂ 3</span> <p class="SimplePara">Summary of classification of frequency-domain metrics regarding computational costs, where <em class="EmphasisTypeItalic ">n</em> is the number of input samples</p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1" /><col class="tcol2" /><col class="tcol3" /><col class="tcol4" /><col class="tcol5" /><col class="tcol6" /><col class="tcol7" /></colgroup><thead><tr><th rowspan="2"> <p class="SimplePara">Frequency-domain metric</p> </th><th colspan="6"> <p class="SimplePara">Computational complexity</p> </th></tr><tr><th> <p class="SimplePara">Add</p> </th><th> <p class="SimplePara">Mult</p> </th><th> <p class="SimplePara">Div</p> </th><th> <p class="SimplePara">Sqrt</p> </th><th> <p class="SimplePara">Log</p> </th><th> <p class="SimplePara">Comparisons</p> </th></tr></thead><tbody><tr><td> <p class="SimplePara">Normalization</p> </td><td> <p class="SimplePara">2<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Energy</p> </td><td> <p class="SimplePara">8<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">6<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em>Â +Â 2</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td></tr><tr><td> <p class="SimplePara">Entropy</p> </td><td> <p class="SimplePara">9<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">6<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em>Â +Â 1</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td></tr><tr><td> <p class="SimplePara">Coeff. sum</p> </td><td> <p class="SimplePara">7<em class="EmphasisTypeItalic ">n</em>Â +Â 5</p> </td><td> <p class="SimplePara">5<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">2<em class="EmphasisTypeItalic ">n</em>Â +Â 1</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Dominant freq.</p> </td><td> <p class="SimplePara">7<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">5<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">2<em class="EmphasisTypeItalic ">n</em>Â +Â 1</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td></tr><tr><td> <p class="SimplePara">Wavelet (<em class="EmphasisTypeItalic ">H</em> <sub>2</sub> and coeff. sum)</p> </td><td> <p class="SimplePara">5<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td></tr></tbody></table></div></div> <div id="Tab4" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">TableÂ 4</span> <p class="SimplePara">Summary of classification of frequency-domain metrics regarding storage costs and memory operations, where <em class="EmphasisTypeItalic ">n</em> is the number of input samples</p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1" /><col class="tcol2" /><col class="tcol3" /><col class="tcol4" /></colgroup><thead><tr><th rowspan="2"> <p class="SimplePara">Frequency-domain metric</p> </th><th rowspan="2"> <p class="SimplePara">Storage</p> <p class="SimplePara">Â </p> </th><th colspan="2"> <p class="SimplePara">Memory</p> </th></tr><tr><th> <p class="SimplePara">Read</p> </th><th> <p class="SimplePara">Write</p> </th></tr></thead><tbody><tr><td> <p class="SimplePara">Normalization</p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td></tr><tr><td> <p class="SimplePara">Energy</p> </td><td> <p class="SimplePara">4<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">4<em class="EmphasisTypeItalic ">n</em> </p> </td></tr><tr><td> <p class="SimplePara">Entropy</p> </td><td> <p class="SimplePara">4<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">4<em class="EmphasisTypeItalic ">n</em> </p> </td></tr><tr><td> <p class="SimplePara">Coeff. sum</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">4<em class="EmphasisTypeItalic ">n</em> </p> </td></tr><tr><td> <p class="SimplePara">Dominant freq.</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">4<em class="EmphasisTypeItalic ">n</em> </p> </td></tr><tr><td> <p class="SimplePara">Wavelet (<em class="EmphasisTypeItalic ">H</em> <sub>2</sub> and coeff. sum)</p> </td><td> <p class="SimplePara">2<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">2<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td></tr></tbody></table></div></div> </div><div class="Para">TablesÂ <span class="InternalRef"><a href="#Tab5">5</a></span> and <span class="InternalRef"><a href="#Tab6">6</a></span> provide an account on the time and space complexity of string-domain metrics. For these metrics, there is no normalization of the input samples as in the case of the time-domain and frequency-domain techniques. However, there is a transformation of representation from continuous values to discrete symbols. This transformation is accomplished by the SAX operation, which is used for all the string-domain metrics and is therefore not included in each specific metric.<div id="Tab5" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">TableÂ 5</span> <p class="SimplePara">Computational complexity of symbolic string-domain metrics where <em class="EmphasisTypeItalic ">n</em> is the number of samples, <em class="EmphasisTypeItalic ">w</em> the number of consecutive samples aggregated in a symbol, and <em class="EmphasisTypeItalic ">a</em> the alphabet size</p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1" /><col class="tcol2" /><col class="tcol3" /><col class="tcol4" /><col class="tcol5" /><col class="tcol6" /></colgroup><thead><tr><th rowspan="2"> <p class="SimplePara">String-domain metric</p> </th><th colspan="5"> <p class="SimplePara">Computational complexity</p> </th></tr><tr><th> <p class="SimplePara">Add</p> </th><th> <p class="SimplePara">Mult</p> </th><th> <p class="SimplePara">Div</p> </th><th> <p class="SimplePara">Sqrt</p> </th><th> <p class="SimplePara">Comparisons</p> </th></tr></thead><tbody><tr><td colspan="6"> <p class="SimplePara">SAX</p> </td></tr><tr><td> <p class="SimplePara">Â Normalization</p> </td><td> <p class="SimplePara">3<em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara"><em class="EmphasisTypeItalic ">n</em>Â +Â 2</p> </td><td> <p class="SimplePara">1</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Â Piecewise approx. (PAA)</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara"><em class="EmphasisTypeItalic ">n</em>/<em class="EmphasisTypeItalic ">w</em></p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Â Gaussian discretization</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara"><em class="EmphasisTypeItalic ">n</em>/<em class="EmphasisTypeItalic ">w</em> ÃÂ <em class="EmphasisTypeItalic ">a</em></p> </td></tr><tr><td> <p class="SimplePara">Minimum distance</p> </td><td> <p class="SimplePara"><em class="EmphasisTypeItalic ">n</em>/<em class="EmphasisTypeItalic ">w</em></p> </td><td> <p class="SimplePara">1</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">1</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Levenshtein</p> </td><td> <p class="SimplePara">3(<em class="EmphasisTypeItalic ">n</em>/<em class="EmphasisTypeItalic ">w</em>)<sup>2</sup> </p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">DTW</p> </td><td> <p class="SimplePara">(<em class="EmphasisTypeItalic ">n</em>/<em class="EmphasisTypeItalic ">w</em>)<sup>2</sup> </p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td><td> <p class="SimplePara">0</p> </td></tr></tbody></table></div></div> <div id="Tab6" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">TableÂ 6</span> <p class="SimplePara">Summary of classification of symbolic string-domain techniques regarding storage costs and memory operations, where <em class="EmphasisTypeItalic ">n</em> is the number of samples, <em class="EmphasisTypeItalic ">w</em> the number of consecutive samples aggregated in a symbol, and <em class="EmphasisTypeItalic ">a</em> the alphabet size</p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1" /><col class="tcol2" /><col class="tcol3" /><col class="tcol4" /></colgroup><thead><tr><th rowspan="2"> <p class="SimplePara">String-domain metric</p> </th><th rowspan="2"> <p class="SimplePara">Storage</p> <p class="SimplePara">Â </p> </th><th colspan="2"> <p class="SimplePara">Memory</p> </th></tr><tr><th> <p class="SimplePara">Read</p> </th><th> <p class="SimplePara">Write</p> </th></tr></thead><tbody><tr><td colspan="4"> <p class="SimplePara">SAX</p> </td></tr><tr><td> <p class="SimplePara">Â Normalization</p> </td><td> <p class="SimplePara">const.</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Â Piecewise approx. (PAA)</p> </td><td> <p class="SimplePara">const.</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">n</em> </p> </td><td> <p class="SimplePara"><em class="EmphasisTypeItalic ">n</em>/<em class="EmphasisTypeItalic ">w</em></p> </td></tr><tr><td> <p class="SimplePara">Â Gaussian discretization</p> </td><td> <p class="SimplePara"><em class="EmphasisTypeItalic ">n</em>/<em class="EmphasisTypeItalic ">w</em>Â +Â <em class="EmphasisTypeItalic ">a</em></p> </td><td> <p class="SimplePara"><em class="EmphasisTypeItalic ">n</em>/<em class="EmphasisTypeItalic ">w</em> ÃÂ <em class="EmphasisTypeItalic ">a</em>/2</p> </td><td> <p class="SimplePara"><em class="EmphasisTypeItalic ">n</em>/2</p> </td></tr><tr><td> <p class="SimplePara">Minimum distance</p> </td><td> <p class="SimplePara"> <em class="EmphasisTypeItalic ">a</em> <sup>2</sup> </p> </td><td> <p class="SimplePara">2(<em class="EmphasisTypeItalic ">n</em>/<em class="EmphasisTypeItalic ">w</em>)</p> </td><td> <p class="SimplePara">0</p> </td></tr><tr><td> <p class="SimplePara">Levenshtein</p> </td><td> <p class="SimplePara">2(<em class="EmphasisTypeItalic ">n</em>/<em class="EmphasisTypeItalic ">w</em>)Â +Â <em class="EmphasisTypeItalic ">a</em> <sup>2</sup> </p> </td><td> <p class="SimplePara">6(<em class="EmphasisTypeItalic ">n</em>/<em class="EmphasisTypeItalic ">w</em>)<sup>2</sup> </p> </td><td> <p class="SimplePara">(<em class="EmphasisTypeItalic ">n</em>/<em class="EmphasisTypeItalic ">w</em>)<sup>2</sup> </p> </td></tr><tr><td> <p class="SimplePara">DTW</p> </td><td> <p class="SimplePara">2(<em class="EmphasisTypeItalic ">n</em>/<em class="EmphasisTypeItalic ">w</em>)Â +Â <em class="EmphasisTypeItalic ">a</em> <sup>2</sup> </p> </td><td> <p class="SimplePara">8(<em class="EmphasisTypeItalic ">n</em>/<em class="EmphasisTypeItalic ">w</em>)<sup>2</sup> </p> </td><td> <p class="SimplePara">(<em class="EmphasisTypeItalic ">n</em>/<em class="EmphasisTypeItalic ">w</em>)<sup>2</sup> </p> </td></tr></tbody></table></div></div> </div><p class="Para">As can be observed, string-domain metrics exhibit much lower costs for expensive operations such as <em class="EmphasisTypeItalic ">sqrt</em> or even multiplications when compared to metrics in the time and frequency domains. For additions, however, and in the case of metrics that rely on dynamic-programming algorithms (<em class="EmphasisTypeItalic ">Levenshtein</em> and <em class="EmphasisTypeItalic ">DTW</em>) the number of operations is quadratic with respect to the <em class="EmphasisTypeItalic ">n</em>/<em class="EmphasisTypeItalic ">w</em> ratio. For long input signals, with large values of <em class="EmphasisTypeItalic ">n</em> and small sample-to-symbol compression ratios (<em class="EmphasisTypeItalic ">w</em>) this value can become quite large. A similar observation holds for memory read and write operations. As to the requirements of additional storage, they are overall fairly low. The only potentially large factor of <em class="EmphasisTypeItalic ">a</em> <sup>2</sup> is due to the alphabet size and thus can be kept in check.</p></section><section id="Sec28" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">2.5.2 </span>Qualitative analysis: suitability for mobile devices</h4><div class="Para">TableÂ <span class="InternalRef"><a href="#Tab7">7</a></span> presents the qualitative results regarding the implementation of each technique on a mobile device. In this qualitative analysis, we label each metric being suitable to be implemented on a mobile device (<span class="Literal"> <span class="EmphasisFontCategoryNonProportional ">Yes</span> </span> label); not being suitable (<span class="Literal"> <span class="EmphasisFontCategoryNonProportional ">No</span> </span> label) and as being moderately suitable (<span class="Literal"> <span class="EmphasisFontCategoryNonProportional ">Moderate</span> </span> label) this last classification corresponding to an implementation that requires either medium or high computing or storage resources.<div id="Tab7" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">TableÂ 7</span> <p class="SimplePara">Summary of classification of time-domain techniques regarding computational costs, storage requirements, and precision (double/single/int)</p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1" /><col class="tcol2" /><col class="tcol3" /><col class="tcol4" /><col class="tcol5" /><col class="tcol6" /></colgroup><thead><tr><th> <p class="SimplePara">Time-domain metric</p> </th><th> <p class="SimplePara">Ref(s)</p> </th><th> <p class="SimplePara">Comp. cost</p> </th><th> <p class="SimplePara">Storage req.</p> </th><th> <p class="SimplePara">Precision</p> </th><th> <p class="SimplePara">Mobile device</p> </th></tr></thead><tbody><tr><td> <p class="SimplePara">Mean</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR5">5</a></span>, <span class="CitationRef"><a href="#CR27">27</a></span>, <span class="CitationRef"><a href="#CR50">50</a></span>, <span class="CitationRef"><a href="#CR59">59</a></span>]</p> </td><td> <p class="SimplePara">Very low</p> </td><td> <p class="SimplePara">Very low</p> </td><td> <p class="SimplePara">Single/int</p> </td><td> <p class="SimplePara">Yes</p> </td></tr><tr><td> <p class="SimplePara">Std. deviation</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR15">15</a></span>, <span class="CitationRef"><a href="#CR22">22</a></span>, <span class="CitationRef"><a href="#CR23">23</a></span>, <span class="CitationRef"><a href="#CR30">30</a></span>, <span class="CitationRef"><a href="#CR59">59</a></span>]</p> </td><td> <p class="SimplePara">Very low</p> </td><td> <p class="SimplePara">Very low</p> </td><td> <p class="SimplePara">Double/single</p> </td><td> <p class="SimplePara">Yes</p> </td></tr><tr><td> <p class="SimplePara">Median</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR2">2</a></span>, <span class="CitationRef"><a href="#CR3">3</a></span>]</p> </td><td> <p class="SimplePara">Medium</p> </td><td> <p class="SimplePara">Very low</p> </td><td> <p class="SimplePara">Single/int</p> </td><td> <p class="SimplePara">Yes</p> </td></tr><tr><td> <p class="SimplePara">Range</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR11">11</a></span>]</p> </td><td> <p class="SimplePara">Very low</p> </td><td> <p class="SimplePara">Very low</p> </td><td> <p class="SimplePara">Single/int</p> </td><td> <p class="SimplePara">Yes</p> </td></tr><tr><td> <p class="SimplePara">Maximum</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR4">4</a></span>, <span class="CitationRef"><a href="#CR59">59</a></span>]</p> </td><td> <p class="SimplePara">Very low</p> </td><td> <p class="SimplePara">Very low</p> </td><td> <p class="SimplePara">Single/int</p> </td><td> <p class="SimplePara">Yes</p> </td></tr><tr><td> <p class="SimplePara">Minimum</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR4">4</a></span>, <span class="CitationRef"><a href="#CR59">59</a></span>]</p> </td><td> <p class="SimplePara">Very low</p> </td><td> <p class="SimplePara">Very low</p> </td><td> <p class="SimplePara">Single/int</p> </td><td> <p class="SimplePara">Yes</p> </td></tr><tr><td> <p class="SimplePara">RMS</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR8">8</a></span>, <span class="CitationRef"><a href="#CR42">42</a></span>, <span class="CitationRef"><a href="#CR53">53</a></span>]</p> </td><td> <p class="SimplePara">Very low</p> </td><td> <p class="SimplePara">Very low</p> </td><td> <p class="SimplePara">Double/single</p> </td><td> <p class="SimplePara">Yes</p> </td></tr><tr><td> <p class="SimplePara">Integration</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR15">15</a></span>, <span class="CitationRef"><a href="#CR30">30</a></span>, <span class="CitationRef"><a href="#CR42">42</a></span>]</p> </td><td> <p class="SimplePara">Very low</p> </td><td> <p class="SimplePara">Very low</p> </td><td> <p class="SimplePara">Double/single</p> </td><td> <p class="SimplePara">Yes</p> </td></tr><tr><td> <p class="SimplePara">Correlation</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR43">43</a></span>, <span class="CitationRef"><a href="#CR57">57</a></span>]</p> </td><td> <p class="SimplePara">Medium</p> </td><td> <p class="SimplePara">Low</p> </td><td> <p class="SimplePara">Double/single</p> </td><td> <p class="SimplePara">Moderate</p> </td></tr><tr><td> <p class="SimplePara">Cross-correlation</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR5">5</a></span>, <span class="CitationRef"><a href="#CR16">16</a></span>, <span class="CitationRef"><a href="#CR18">18</a></span>, <span class="CitationRef"><a href="#CR28">28</a></span>]</p> </td><td> <p class="SimplePara">Medium</p> </td><td> <p class="SimplePara">Low</p> </td><td> <p class="SimplePara">Double/single</p> </td><td> <p class="SimplePara">Moderate</p> </td></tr><tr><td> <p class="SimplePara">Differences</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR55">55</a></span>]</p> </td><td> <p class="SimplePara">Very low</p> </td><td> <p class="SimplePara">Very low</p> </td><td> <p class="SimplePara">Single/int</p> </td><td> <p class="SimplePara">Yes</p> </td></tr><tr><td> <p class="SimplePara">Zero-crossings</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR8">8</a></span>, <span class="CitationRef"><a href="#CR11">11</a></span>, <span class="CitationRef"><a href="#CR30">30</a></span>, <span class="CitationRef"><a href="#CR49">49</a></span>]</p> </td><td> <p class="SimplePara">Very low</p> </td><td> <p class="SimplePara">Very low</p> </td><td> <p class="SimplePara">Single/int</p> </td><td> <p class="SimplePara">Yes</p> </td></tr><tr><td> <p class="SimplePara">SMA</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR6">6</a></span>, <span class="CitationRef"><a href="#CR21">21</a></span>, <span class="CitationRef"><a href="#CR37">37</a></span>, <span class="CitationRef"><a href="#CR38">38</a></span>]</p> </td><td> <p class="SimplePara">Low</p> </td><td> <p class="SimplePara">Low</p> </td><td> <p class="SimplePara">Single/int</p> </td><td> <p class="SimplePara">Yes</p> </td></tr><tr><td> <p class="SimplePara">SVM</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR21">21</a></span>, <span class="CitationRef"><a href="#CR44">44</a></span>]</p> </td><td> <p class="SimplePara">Low</p> </td><td> <p class="SimplePara">Low</p> </td><td> <p class="SimplePara">Double/single</p> </td><td> <p class="SimplePara">Yes</p> </td></tr><tr><td> <p class="SimplePara">DSVM</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR19">19</a></span>]</p> </td><td> <p class="SimplePara">Low</p> </td><td> <p class="SimplePara">Low</p> </td><td> <p class="SimplePara">Double/single</p> </td><td> <p class="SimplePara">Yes</p> </td></tr></tbody></table></div></div> </div><div class="Para">To aid and distinguish this classification we further label each metric with four cost-indication levels, namely:<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li> <p class="Para">A metric exhibits a <em class="EmphasisTypeItalic ">very low</em> computational cost if it requires only a number of operations that have a linear relation to the number of input samples <em class="EmphasisTypeItalic ">n</em> and these operations are in its vast majority arithmetic additions and subtractions.</p> </li><li> <p class="Para">A metric exhibits a <em class="EmphasisTypeItalic ">low</em> computational cost if it requires a number of operations that has a linear relation to the number of input samples <em class="EmphasisTypeItalic ">n</em>, which will include multiplications and division. A fixed number of operations can be advanced arithmetic operations such as square-root or logarithm.</p> </li><li> <p class="Para">A <em class="EmphasisTypeItalic ">medium</em> computational cost will include metrics that are quadratic in terms of the number of input samples <em class="EmphasisTypeItalic ">n</em> of simple addition/subtraction or multiplication/division operations. A fixed number of operations can be advanced arithmetic operations such as square-root or logarithm.</p> </li><li> <p class="Para">A <em class="EmphasisTypeItalic ">high</em> computational cost will include techniques that require a number of operations larger than an asymptotic quadratic bound, but where the individual operations are simple arithmetic additions/subtractions or multiplications/division. In this category, a linear number of advanced operations such as <em class="EmphasisTypeItalic ">sine</em> or <em class="EmphasisTypeItalic ">log</em> are required.</p> </li></ul></div> </div><p class="Para">In addition to this implementation complexity, we also indicate the category of precision, and we include the base data types required in common implementations of these techniques. While in some cases it is possible to use basic data types with less precision (e.g., use integer values in place of <em class="EmphasisTypeItalic ">doubles</em>), typically the accuracy of the specific metric can degrade substantially. For this reason, in some cases, we include more than one base data type.</p><div class="Para">TableÂ <span class="InternalRef"><a href="#Tab8">8</a></span> summarizes the results of the qualitative analysis for the frequency-domain metrics. With the exception of metrics based on Wavelet transforms, all others rely on the computation of the signal spectrum. As such they are deemed expensive in terms of computational cost but still requiring a very modest amount of storage. Given the sophistication of the computations, the required precision is typically high, either using double or using  single precision arithmetic. Only the metrics based on Wavelets and those using simple arithmetics or transformations are classified as requiring a low computational effort.<sup><a href="#Fn3" id="Fn3_source">3</a></sup> <div id="Tab8" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">TableÂ 8</span> <p class="SimplePara">Summary of classification of frequency-domain techniques regarding computational costs, storage requirements and precision (double/single/int)</p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1" /><col class="tcol2" /><col class="tcol3" /><col class="tcol4" /><col class="tcol5" /><col class="tcol6" /></colgroup><thead><tr><th> <p class="SimplePara">Frequency-domain metric</p> </th><th> <p class="SimplePara">Ref(s)</p> </th><th> <p class="SimplePara">Comp. cost</p> </th><th> <p class="SimplePara">Storage req.</p> </th><th> <p class="SimplePara">Precision</p> </th><th> <p class="SimplePara">Mobile device</p> </th></tr></thead><tbody><tr><td> <p class="SimplePara">Energy</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR5">5</a></span>, <span class="CitationRef"><a href="#CR16">16</a></span>, <span class="CitationRef"><a href="#CR18">18</a></span>, <span class="CitationRef"><a href="#CR27">27</a></span>, <span class="CitationRef"><a href="#CR28">28</a></span>, <span class="CitationRef"><a href="#CR41">41</a></span>]</p> </td><td> <p class="SimplePara">Medium</p> </td><td> <p class="SimplePara">Low</p> </td><td> <p class="SimplePara">Double/single</p> </td><td> <p class="SimplePara">Moderate</p> </td></tr><tr><td> <p class="SimplePara">Entropy</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR5">5</a></span>, <span class="CitationRef"><a href="#CR16">16</a></span>, <span class="CitationRef"><a href="#CR18">18</a></span>, <span class="CitationRef"><a href="#CR28">28</a></span>]</p> </td><td> <p class="SimplePara">High</p> </td><td> <p class="SimplePara">Low</p> </td><td> <p class="SimplePara">Double/single</p> </td><td> <p class="SimplePara">No</p> </td></tr><tr><td> <p class="SimplePara">Coeff. sum</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR62">62</a></span>]</p> </td><td> <p class="SimplePara">Medium</p> </td><td> <p class="SimplePara">Low</p> </td><td> <p class="SimplePara">Double/single</p> </td><td> <p class="SimplePara">Moderate</p> </td></tr><tr><td> <p class="SimplePara">Dominant freq.</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR21">21</a></span>, <span class="CitationRef"><a href="#CR22">22</a></span>, <span class="CitationRef"><a href="#CR23">23</a></span>, <span class="CitationRef"><a href="#CR24">24</a></span>, <span class="CitationRef"><a href="#CR37">37</a></span>]</p> </td><td> <p class="SimplePara">Medium</p> </td><td> <p class="SimplePara">Low</p> </td><td> <p class="SimplePara">Double/single</p> </td><td> <p class="SimplePara">Moderate</p> </td></tr><tr><td> <p class="SimplePara">Wavelet (<em class="EmphasisTypeItalic ">H</em> <sub>2</sub> and coeff. sum)</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR34">34</a></span>]</p> </td><td> <p class="SimplePara">Low</p> </td><td> <p class="SimplePara">Low</p> </td><td> <p class="SimplePara">Double/single</p> </td><td> <p class="SimplePara">Yes</p> </td></tr></tbody></table></div></div> </div><div class="Para">TableÂ <span class="InternalRef"><a href="#Tab9">9</a></span> summarizes the results of the qualitative analysis for the symbolic string-domain metrics. With the exception of the DTW metric, these string-based metrics require a fairly low computational cost as they do not rely on sophisticated arithmetic and usually require only integer arithmetic. Regarding storage, however, dynamic-programming metrics such as the Levenshtein or the DTW may require storage that is quadratic on the length of their input strings.<sup><a href="#Fn4" id="Fn4_source">4</a></sup> This apparent high-storage requirement is balanced by the fact that symbol-string representations lend themselves to substantial compression rates. As a result, these metrics are very suitable for direct implementation on mobile devices.<div id="Tab9" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">TableÂ 9</span> <p class="SimplePara">Summary of classification of symbolic string-domain techniques regarding computational costs, storage requirements, and precision (double/single/int)</p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1" /><col class="tcol2" /><col class="tcol3" /><col class="tcol4" /><col class="tcol5" /><col class="tcol6" /></colgroup><thead><tr><th> <p class="SimplePara">String-domain metric</p> </th><th> <p class="SimplePara">Ref(s)</p> </th><th> <p class="SimplePara">Comp. cost</p> </th><th> <p class="SimplePara">Storage req.</p> </th><th> <p class="SimplePara">Precision</p> </th><th> <p class="SimplePara">Mobile device</p> </th></tr></thead><tbody><tr><td> <p class="SimplePara">Minimum distance</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR31">31</a></span>]</p> </td><td> <p class="SimplePara">Low</p> </td><td> <p class="SimplePara">Low</p> </td><td> <p class="SimplePara">Int</p> </td><td> <p class="SimplePara">Yes</p> </td></tr><tr><td> <p class="SimplePara">Levenshtein</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR14">14</a></span>]</p> </td><td> <p class="SimplePara">Medium</p> </td><td> <p class="SimplePara">Medium</p> </td><td> <p class="SimplePara">Int</p> </td><td> <p class="SimplePara">Moderate</p> </td></tr><tr><td> <p class="SimplePara">DTW</p> </td><td> <p class="SimplePara">[<span class="CitationRef"><a href="#CR46">46</a></span>]</p> </td><td> <p class="SimplePara">Medium</p> </td><td> <p class="SimplePara">Medium</p> </td><td> <p class="SimplePara">Int</p> </td><td> <p class="SimplePara">Moderate</p> </td></tr></tbody></table></div></div> </div></section></section></div></section><section id="Sec29" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">3 </span>Experimental evaluation</h2><div class="content"><p class="Para">In this section, we discuss and evaluate the suitability of the techniques described in Sect. <span class="InternalRef"><a href="#Sec2">2</a></span> in recognizing specific user activities based on preprocessing the raw data coming from a three-axis accelerometer. We begin by a description of the way we collected the input data and of the way the various techniques have been implemented. Afterward, we present the experimental results and discuss the accuracy achieved in recognizing user activity in two scenarios: one where the goal is to be able to distinguish between three activities (namely <em class="EmphasisTypeItalic ">walking, running</em> and <em class="EmphasisTypeItalic ">jumping</em>) and another scenario where only two activities are considered (<em class="EmphasisTypeItalic ">walking</em> and <em class="EmphasisTypeItalic ">running</em>). We will refer to these as the <em class="EmphasisTypeItalic ">three-activity</em> and the <em class="EmphasisTypeItalic ">two-activity</em> scenarios, respectively.</p><div class="Para">The three possible activities are depicted in Fig.Â <span class="InternalRef"><a href="#Fig3">3</a></span>a, b and c, where the accelerometer device is visible in the right-hand pocket. By attempting to distinguish between three activities, our experiments differ from other applications of the same methods, where typically only a binary classification such as <em class="EmphasisTypeItalic ">active</em> and <em class="EmphasisTypeItalic ">non-active</em> is attained.<figure class="Figure" id="Fig3"><div class="MediaObject" id="MO12"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs00779-010-0293-9/MediaObjects/779_2010_293_Fig3_HTML.jpg" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="https://media.springernature.com/lw785/springer-static/image/art%3A10.1007%2Fs00779-010-0293-9/MediaObjects/779_2010_293_Fig3_HTML.jpg" alt="Fig.Â 3" /></a> </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig.Â 3</span> <p class="SimplePara">Data collection illustration for the three activities, namely <strong class="EmphasisTypeBold ">a</strong> <em class="EmphasisTypeItalic ">walking</em>, <strong class="EmphasisTypeBold ">b</strong> <em class="EmphasisTypeItalic ">running</em> and <strong class="EmphasisTypeBold ">c</strong> <em class="EmphasisTypeItalic ">jumping</em>, with a Wii Remote connected to a laptop for recording <strong class="EmphasisTypeBold ">d</strong> </p> </div></figcaption></figure> </div><section id="Sec30" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">3.1 </span>Evaluation methodology</h3><p class="Para">For the evaluation of different preprocessing techniques, we collected raw sensor data using a <span class="InlineEquation" id="IEq3">\(\hbox{Nintendo}^{\circledR}\)</span> Wii Remote (generally known as <em class="EmphasisTypeItalic ">Wiimote</em>) shown in Fig.Â <span class="InternalRef"><a href="#Fig3">3</a></span>d. The Wii Remote includes a three-axis accelerometer sensor (an ADXL330 chip from Analog Devices) that delivers up to 100 data samples per second, where each sample contains the values for the three accelerometer axes. It has built-in bluetooth communication capabilities, which were used to record the data directly to a laptop.</p><p class="Para">The accelerometer sensor measures acceleration with a minimum full-scale range of 3<em class="EmphasisTypeItalic ">g</em> and it senses both the gravity pull and the acceleration resulting from motion, shock or vibration. Raw numeric data values for each axis range from 0 (â3<em class="EmphasisTypeItalic ">g</em>) to 255 (+3<em class="EmphasisTypeItalic ">g</em>) with the value 127 corresponding to zero acceleration.</p><div class="Para">With this acquisition setup, we collected raw data for the three activities (<em class="EmphasisTypeItalic ">walking, running</em> and <em class="EmphasisTypeItalic ">jumping</em>) performed by volunteer students. Several minutes were collected for each activity and then split into separate files of 60Â s each. As an illustration, Fig.Â <span class="InternalRef"><a href="#Fig4">4</a></span> shows 60Â s of <em class="EmphasisTypeItalic ">jumping</em>, 60Â s of <em class="EmphasisTypeItalic ">running</em> and 60Â s of <em class="EmphasisTypeItalic ">walking</em> concatenated together.<figure class="Figure" id="Fig4"><div class="MediaObject" id="MO13"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs00779-010-0293-9/MediaObjects/779_2010_293_Fig4_HTML.gif" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="https://media.springernature.com/lw785/springer-static/image/art%3A10.1007%2Fs00779-010-0293-9/MediaObjects/779_2010_293_Fig4_HTML.gif" alt="Fig.Â 4" /></a> </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig.Â 4</span> <p class="SimplePara">Plot of a 60-s file for each activity, each file containing approximately 6000 samples</p> </div></figcaption></figure> </div><div class="Para">For these simple activities, each 60-s file has only one activity in its âpureâ form, meaning that it is a full minute of either <em class="EmphasisTypeItalic ">walking, running</em> or <em class="EmphasisTypeItalic ">jumping</em>. There were a total of 15 files for <em class="EmphasisTypeItalic ">walking</em>, 9 files for <em class="EmphasisTypeItalic ">running</em> and 6 files for <em class="EmphasisTypeItalic ">jumping</em>. These files have been divided into a <em class="EmphasisTypeItalic ">training set</em> and a <em class="EmphasisTypeItalic ">test set</em>, as shown in TableÂ <span class="InternalRef"><a href="#Tab10">10</a></span>. The files in the training set were used to empirically tune the parameters of each algorithm, while the files in the test set were used as independent samples to test the actual accuracy of the trained algorithm.<div id="Tab10" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">TableÂ 10</span> <p class="SimplePara">Activities used in experimental evaluation</p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1" /><col class="tcol2 align-char" /><col class="tcol3 align-char" /><col class="tcol4 align-char" /><col class="tcol5 align-char" /><col class="tcol6 align-char" /><col class="tcol7 align-char" /></colgroup><thead><tr><th rowspan="2"> <p class="SimplePara">Activities</p> </th><th colspan="2"> <p class="SimplePara">Training set</p> </th><th colspan="2"> <p class="SimplePara">Test set</p> </th><th colspan="2"> <p class="SimplePara">Total</p> </th></tr><tr><th> <p class="SimplePara">Files</p> </th><th> <p class="SimplePara">Windows</p> </th><th> <p class="SimplePara">Files</p> </th><th> <p class="SimplePara">Windows</p> </th><th> <p class="SimplePara">Files</p> </th><th> <p class="SimplePara">Windows</p> </th></tr></thead><tbody><tr><td> <p class="SimplePara">Walking</p> </td><td> <p class="SimplePara">7</p> </td><td> <p class="SimplePara">311</p> </td><td> <p class="SimplePara">8</p> </td><td> <p class="SimplePara">355</p> </td><td> <p class="SimplePara">15</p> </td><td> <p class="SimplePara">666</p> </td></tr><tr><td> <p class="SimplePara">Running</p> </td><td> <p class="SimplePara">4</p> </td><td> <p class="SimplePara">175</p> </td><td> <p class="SimplePara">5</p> </td><td> <p class="SimplePara">214</p> </td><td> <p class="SimplePara">9</p> </td><td> <p class="SimplePara">389</p> </td></tr><tr><td> <p class="SimplePara">Jumping</p> </td><td> <p class="SimplePara">3</p> </td><td> <p class="SimplePara">135</p> </td><td> <p class="SimplePara">3</p> </td><td> <p class="SimplePara">136</p> </td><td> <p class="SimplePara">6</p> </td><td> <p class="SimplePara">271</p> </td></tr></tbody></table></div></div> </div><p class="Para">We report the results of activity recognition for both the two-activity and the three-activity scenario. Accuracy is computed as the percentage of correctly classified files from the test set, for all activities. The same measure of accuracy has been computed separately for the files in the training set in order to provide an idea of the maximum accuracy that could be possibly achieved. For the training set, the accuracy is seldom 100% as each algorithm needs to distinguish the files of all activities, and the best choice of the algorithm parameters (as described below) may not allow for a complete discrimination.</p><section id="Sec31" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">3.1.1 </span>Selection of threshold parameters</h4><p class="Para">Given the sampling frequency of the accelerometer (â¤100Â Hz) each 60-s file contains 6,000 samples each of which includes data from each of the three axis. The data in each of these files are further divided into windows of 256 samples, each window with a 50% overlap with the previous one. The 256-sample window is the basic unit to be classified by a given technique.</p><div class="Para">In order to carry out the evaluation experiments, we have implemented the techniques described in Sect. <span class="InternalRef"><a href="#Sec2">2</a></span> in <span class="InlineEquation" id="IEq4">\(\hbox{MATLAB}^{\circledR}\)</span> according either to the reference article or to the mathematical definition of each technique. For most metrics, the first step is to compute the norm <em class="EmphasisTypeItalic ">n</em> <sub> <em class="EmphasisTypeItalic ">i</em> </sub> for each individual sample (<em class="EmphasisTypeItalic ">x</em> <sub> <em class="EmphasisTypeItalic ">i</em> </sub>;<em class="EmphasisTypeItalic ">y</em> <sub> <em class="EmphasisTypeItalic ">i</em> </sub>;<em class="EmphasisTypeItalic ">z</em> <sub> <em class="EmphasisTypeItalic ">i</em> </sub>), i.e., <span class="InlineEquation" id="IEq2">\(n_i = \sqrt{{x_i}^2+{y_i}^2+{z_i}^2}\)</span> and then apply the chosen metric to the resulting normal signal. A second step consists in finding the two thresholds parametersâ<em class="EmphasisTypeItalic ">thr</em> <sub>1</sub> and <em class="EmphasisTypeItalic ">thr</em> <sub>2</sub>âthat separate the activities based on the chosen metric. With three activities, there is always one threshold (<em class="EmphasisTypeItalic ">thr</em> <sub>1</sub>) separating the lower-valued class from the intermediate one and another threshold (<em class="EmphasisTypeItalic ">thr</em> <sub>2</sub>) separating the intermediate class to the higher-valued one. In the example of Fig.Â <span class="InternalRef"><a href="#Fig5">5</a></span>, <em class="EmphasisTypeItalic ">thr</em> <sub>1</sub> separates <em class="EmphasisTypeItalic ">walking</em> from <em class="EmphasisTypeItalic ">running</em> and <em class="EmphasisTypeItalic ">thr</em> <sub>2</sub> separates <em class="EmphasisTypeItalic ">running</em> from <em class="EmphasisTypeItalic ">jumping</em>.<figure class="Figure" id="Fig5"><div class="MediaObject" id="MO14"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs00779-010-0293-9/MediaObjects/779_2010_293_Fig5_HTML.gif" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="https://media.springernature.com/lw785/springer-static/image/art%3A10.1007%2Fs00779-010-0293-9/MediaObjects/779_2010_293_Fig5_HTML.gif" alt="Fig.Â 5" /></a> </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig.Â 5</span> <p class="SimplePara">For any given metric, two thresholds <em class="EmphasisTypeItalic ">thr</em> <sub>1</sub> and <em class="EmphasisTypeItalic ">thr</em> <sub>2</sub> can be used to separate the three classes of user activities</p> </div></figcaption></figure> </div><p class="Para">The configuration of these thresholds may differ depending on the results of the chosen metric, so in the first step, it is necessary to pick up one file from each activity and identify the classes that the two thresholds will separate. Then, the second step consists in taking the whole training set and find the best values of <em class="EmphasisTypeItalic ">thr</em> <sub>1</sub> and <em class="EmphasisTypeItalic ">thr</em> <sub>2</sub>, where <em class="EmphasisTypeItalic ">thr</em> <sub>1</sub>Â &lt;Â <em class="EmphasisTypeItalic ">th</em> <sub>2</sub>. The optimal values for these parameters are obtained when there is maximum accuracy in classifying the training set, i.e., when most (if not all) 256-sample windows are correctly identified as belonging to the activity that is specified in the training set.</p><div class="Para">In our implementation, we obtained the values of <em class="EmphasisTypeItalic ">thr</em> <sub>1</sub> and <em class="EmphasisTypeItalic ">thr</em> <sub>2</sub> for each metric by sweeping through the value range using a bisection method over the search space, until the accuracy can be improved no further. An example of such search on a space grid is shown in Fig.Â <span class="InternalRef"><a href="#Fig6">6</a></span>. In this case, we have a metric that varies between 0.0 and 1.0, and the best result is achieved when <em class="EmphasisTypeItalic ">thr</em> <sub>1</sub>Â =Â 0.4 and <em class="EmphasisTypeItalic ">thr</em> <sub>2</sub>Â =Â 0.5, respectively. In this work, however, we did not explore the impact of window size and of the degree of overlap, as has been studied by other authors [<span class="CitationRef"><a href="#CR17">17</a></span>].<figure class="Figure" id="Fig6"><div class="MediaObject" id="MO15"> <a href="https://media.springernature.com/original/springer-static/image/art%3A10.1007%2Fs00779-010-0293-9/MediaObjects/779_2010_293_Fig6_HTML.gif" target="_blank" rel="noopener"><span class="u-screenreader-only">Open image in new window</span><img src="https://media.springernature.com/lw785/springer-static/image/art%3A10.1007%2Fs00779-010-0293-9/MediaObjects/779_2010_293_Fig6_HTML.gif" alt="Fig.Â 6" /></a> </div><figcaption class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">Fig.Â 6</span> <p class="SimplePara">Illustration of the selection of best threshold parameters for the correlation metric</p> </div></figcaption></figure> </div></section><section id="Sec32" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">3.1.2 </span>Accuracy in activity recognition</h4><p class="Para">After the threshold parameters <em class="EmphasisTypeItalic ">thr</em> <sub>1</sub> and <em class="EmphasisTypeItalic ">thr</em> <sub>2</sub> have been determined, training is complete, and it is time to apply the chosen metric to the test set in order to evaluate its real accuracy.</p><div class="Para">Testing for the accuracy of a given metric is straightforward: one has only to pick each window in the test set, compute its metric value, and compare this value to the trained thresholds. Since the two thresholds define three classes of activities, we have:
<div class="UnorderedList"><ul class="UnorderedListMarkBullet"><li> <p class="Para">if the metric value is below <em class="EmphasisTypeItalic ">thr</em> <sub>1</sub>, the window is classified in the lower-class;</p> </li><li> <p class="Para">if the value is between <em class="EmphasisTypeItalic ">thr</em> <sub>1</sub> and <em class="EmphasisTypeItalic ">thr</em> <sub>2</sub>, the window is classified in the middle-class;</p> </li><li> <p class="Para">if the metric value is above <em class="EmphasisTypeItalic ">thr</em> <sub>2</sub>, the window is classified in the higher-class.</p> </li></ul></div> </div><p class="Para">The correspondence between classes and activities has been established previously during the training phase. The overall test accuracy is obtained by classifying all windows in the test set. This accuracy is defined as the percentage of windows that have been correctly assigned to their true activity.</p></section></section><section id="Sec33" tabindex="-1" class="Section2 RenderAsSection2"><h3 class="Heading"><span class="HeadingNumber">3.2 </span>Experimental results</h3><p class="Para">We now present the evaluation results for the preprocessing techniques described in Sect. <span class="InternalRef"><a href="#Sec2">2</a></span>. The evaluation is carried out both for the three-activity scenario and for the two-activity scenario. For the two-activity scenario, the approach is basically the same but with a single threshold parameter.</p><section id="Sec34" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">3.2.1 </span>Results for time-domain metrics</h4><div class="Para">TableÂ <span class="InternalRef"><a href="#Tab11">11</a></span> presents the results for the time-domain metrics in the three-activity scenario. The threshold values <em class="EmphasisTypeItalic ">thr</em> <sub>1</sub> and <em class="EmphasisTypeItalic ">thr</em> <sub>2</sub> that have been derived from the training set are shown, together with the accuracy obtained both in the training set and in the test set.<div id="Tab11" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">TableÂ 11</span> <p class="SimplePara">Summary of the results obtained by applying a selected set of the time-domain techniques to collected data for three activities <em class="EmphasisTypeItalic ">walking, running</em> and <em class="EmphasisTypeItalic ">jumping</em> </p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1" /><col class="tcol2 align-left" /><col class="tcol3 align-char" /><col class="tcol4 align-char" /><col class="tcol5 align-char" /></colgroup><thead><tr><th rowspan="2"> <p class="SimplePara">Time-domain metric</p> </th><th colspan="2"> <p class="SimplePara">Parameter values</p> </th><th rowspan="2"> <p class="SimplePara">Training accuracy (%)</p> </th><th rowspan="2"> <p class="SimplePara">Test accuracy (%)</p> </th></tr><tr><th> <p class="SimplePara"> <em class="EmphasisTypeItalic ">thr</em> <sub>1</sub> </p> </th><th> <p class="SimplePara"> <em class="EmphasisTypeItalic ">thr</em> <sub>2</sub> </p> </th></tr></thead><tbody><tr><td> <p class="SimplePara">Mean [<span class="CitationRef"><a href="#CR5">5</a></span>]</p> </td><td> <p class="SimplePara">228.4</p> </td><td> <p class="SimplePara">241.1</p> </td><td> <p class="SimplePara">67.96</p> </td><td> <p class="SimplePara">59.14</p> </td></tr><tr><td> <p class="SimplePara">Std. deviation [<span class="CitationRef"><a href="#CR23">23</a></span>]</p> </td><td> <p class="SimplePara">17.7</p> </td><td> <p class="SimplePara">28</p> </td><td> <p class="SimplePara">63.74</p> </td><td> <p class="SimplePara">58.54</p> </td></tr><tr><td> <p class="SimplePara">Median [<span class="CitationRef"><a href="#CR2">2</a></span>]</p> </td><td> <p class="SimplePara">226.5</p> </td><td> <p class="SimplePara">242.2</p> </td><td> <p class="SimplePara">60.37</p> </td><td> <p class="SimplePara">56.02</p> </td></tr><tr><td> <p class="SimplePara">Range [<span class="CitationRef"><a href="#CR11">11</a></span>]</p> </td><td> <p class="SimplePara">63</p> </td><td> <p class="SimplePara">95.7</p> </td><td> <p class="SimplePara">64.24</p> </td><td> <p class="SimplePara">70.73</p> </td></tr><tr><td> <p class="SimplePara">Maximum [<span class="CitationRef"><a href="#CR59">59</a></span>]</p> </td><td> <p class="SimplePara">264.3</p> </td><td> <p class="SimplePara">296</p> </td><td> <p class="SimplePara">71.16</p> </td><td> <p class="SimplePara">67.01</p> </td></tr><tr><td> <p class="SimplePara">Minimum [<span class="CitationRef"><a href="#CR59">59</a></span>]</p> </td><td> <p class="SimplePara">200.2</p> </td><td> <p class="SimplePara">211.5</p> </td><td> <p class="SimplePara">80.10</p> </td><td> <p class="SimplePara">80.39</p> </td></tr><tr><td> <p class="SimplePara">RMS [<span class="CitationRef"><a href="#CR42">42</a></span>]</p> </td><td> <p class="SimplePara">228.1</p> </td><td> <p class="SimplePara">237</p> </td><td> <p class="SimplePara">65.76</p> </td><td> <p class="SimplePara">46.81</p> </td></tr><tr><td> <p class="SimplePara">Integration [<span class="CitationRef"><a href="#CR42">42</a></span>]</p> </td><td> <p class="SimplePara">58,246</p> </td><td> <p class="SimplePara">60,372</p> </td><td> <p class="SimplePara">66.10</p> </td><td> <p class="SimplePara">46.95</p> </td></tr><tr><td> <p class="SimplePara">Correlation (<em class="EmphasisTypeItalic ">x</em>,Â <em class="EmphasisTypeItalic ">y</em>) [<span class="CitationRef"><a href="#CR43">43</a></span>]</p> </td><td> <p class="SimplePara">â0.33</p> </td><td> <p class="SimplePara">â0.78</p> </td><td> <p class="SimplePara">70.32</p> </td><td> <p class="SimplePara">68.05</p> </td></tr><tr><td> <p class="SimplePara">Cross-correlation (<em class="EmphasisTypeItalic ">x</em>,Â <em class="EmphasisTypeItalic ">y</em>) [<span class="CitationRef"><a href="#CR5">5</a></span>]</p> </td><td> <p class="SimplePara">449</p> </td><td> <p class="SimplePara">511</p> </td><td> <p class="SimplePara">70.82</p> </td><td> <p class="SimplePara">57.06</p> </td></tr><tr><td> <p class="SimplePara">Differences [<span class="CitationRef"><a href="#CR55">55</a></span>]</p> </td><td> <p class="SimplePara">5.3</p> </td><td> <p class="SimplePara">8.2</p> </td><td> <p class="SimplePara">97.30</p> </td><td> <p class="SimplePara">80.83</p> </td></tr><tr><td> <p class="SimplePara">Zero-crossings [<span class="CitationRef"><a href="#CR11">11</a></span>]</p> </td><td> <p class="SimplePara">16.8</p> </td><td> <p class="SimplePara">36.6</p> </td><td> <p class="SimplePara">76.50</p> </td><td> <p class="SimplePara">74.44</p> </td></tr><tr><td> <p class="SimplePara">SMA [<span class="CitationRef"><a href="#CR21">21</a></span>]</p> </td><td> <p class="SimplePara">401</p> </td><td> <p class="SimplePara">417</p> </td><td> <p class="SimplePara">68.63</p> </td><td> <p class="SimplePara">40.56</p> </td></tr><tr><td> <p class="SimplePara">SVM [<span class="CitationRef"><a href="#CR21">21</a></span>]</p> </td><td> <p class="SimplePara">228</p> </td><td> <p class="SimplePara">241</p> </td><td> <p class="SimplePara">67.45</p> </td><td> <p class="SimplePara">60.33</p> </td></tr><tr><td> <p class="SimplePara">DSVM [<span class="CitationRef"><a href="#CR19">19</a></span>]</p> </td><td> <p class="SimplePara">58,280</p> </td><td> <p class="SimplePara">60,290</p> </td><td> <p class="SimplePara">66.27</p> </td><td> <p class="SimplePara">46.36</p> </td></tr></tbody></table></div></div> </div><p class="Para">Overall, the techniques exhibit a wide range of accuracy results with an average of about 70.4% for the training set and of 60.8% for the test set. Of all these metrics, the <em class="EmphasisTypeItalic ">differences</em> metric seems to be by far the most accurate one, followed by the <em class="EmphasisTypeItalic ">minimum</em> metric. All other metrics exhibit substantially less accuracy particularly for the test set. The case of the <em class="EmphasisTypeItalic ">differences</em> metric is somewhat misleading as the implementation relies on other metrics to increase its accuracy. Specifically, it makes use of an average over the window to eliminate spikes. The combination of these metrics (<em class="EmphasisTypeItalic ">differences</em> and <em class="EmphasisTypeItalic ">mean</em>) achieves an impressive 97.3% accuracy rate. Lastly, we note that metrics such as <em class="EmphasisTypeItalic ">SMA</em> and <em class="EmphasisTypeItalic ">DSVM</em> have a somewhat disappointing performance given the complexity of their implementation. The <em class="EmphasisTypeItalic ">RMS</em> and the <em class="EmphasisTypeItalic ">integration</em> metric also seem to be particularly ineffective in the test set.</p><div class="Para">TableÂ <span class="InternalRef"><a href="#Tab12">12</a></span> presents the results for the time-domain metrics in the two-activity scenario. As expected, given the simpler recognition task, the metrics exhibit in general better results than for the three-activity scenario. The <em class="EmphasisTypeItalic ">differences</em> and the <em class="EmphasisTypeItalic ">minimum</em> metrics are again the most accurate, with the <em class="EmphasisTypeItalic ">differences</em> metric achieving a perfect fit to the training set. For both metrics, the accuracy is quite high in the test set. When comparing against the results for the three-activity scenario, the metrics in general allow for a higher accuracy with an average of 79.7 and 70.1%, accuracy rate in the training set and in the test set, respectively.<div id="Tab12" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">TableÂ 12</span> <p class="SimplePara">Summary of the results obtained by applying a selected set of the time-domain techniques to collected data for two activities of <em class="EmphasisTypeItalic ">walking</em> and <em class="EmphasisTypeItalic ">running</em> </p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1" /><col class="tcol2 align-left" /><col class="tcol3 align-char" /><col class="tcol4 align-char" /></colgroup><thead><tr><th rowspan="2"> <p class="SimplePara">Time-domain metric</p> </th><th> <p class="SimplePara">Parameter value</p> </th><th rowspan="2"> <p class="SimplePara">Training accuracy (%)</p> </th><th rowspan="2"> <p class="SimplePara">Test accuracy (%)</p> </th></tr><tr><th> <p class="SimplePara"> <em class="EmphasisTypeItalic ">thr</em> <sub>1</sub> </p> </th></tr></thead><tbody><tr><td> <p class="SimplePara">Mean [<span class="CitationRef"><a href="#CR5">5</a></span>]</p> </td><td> <p class="SimplePara">233.9</p> </td><td> <p class="SimplePara">76.94</p> </td><td> <p class="SimplePara">52.22</p> </td></tr><tr><td> <p class="SimplePara">Std. deviation [<span class="CitationRef"><a href="#CR23">23</a></span>]</p> </td><td> <p class="SimplePara">14.7</p> </td><td> <p class="SimplePara">82.97</p> </td><td> <p class="SimplePara">81.95</p> </td></tr><tr><td> <p class="SimplePara">Median [<span class="CitationRef"><a href="#CR2">2</a></span>]</p> </td><td> <p class="SimplePara">231.7</p> </td><td> <p class="SimplePara">76.72</p> </td><td> <p class="SimplePara">53.96</p> </td></tr><tr><td> <p class="SimplePara">Range [<span class="CitationRef"><a href="#CR11">11</a></span>]</p> </td><td> <p class="SimplePara">109.5</p> </td><td> <p class="SimplePara">70.47</p> </td><td> <p class="SimplePara">68.51</p> </td></tr><tr><td> <p class="SimplePara">Maximum [<span class="CitationRef"><a href="#CR59">59</a></span>]</p> </td><td> <p class="SimplePara">264.3</p> </td><td> <p class="SimplePara">64.22</p> </td><td> <p class="SimplePara">71.60</p> </td></tr><tr><td> <p class="SimplePara">Minimum [<span class="CitationRef"><a href="#CR59">59</a></span>]</p> </td><td> <p class="SimplePara">205.8</p> </td><td> <p class="SimplePara">98.60</p> </td><td> <p class="SimplePara">95.95</p> </td></tr><tr><td> <p class="SimplePara">RMS [<span class="CitationRef"><a href="#CR42">42</a></span>]</p> </td><td> <p class="SimplePara">233.7</p> </td><td> <p class="SimplePara">75.43</p> </td><td> <p class="SimplePara">53.41</p> </td></tr><tr><td> <p class="SimplePara">Integration [<span class="CitationRef"><a href="#CR42">42</a></span>]</p> </td><td> <p class="SimplePara">59,655</p> </td><td> <p class="SimplePara">76.94</p> </td><td> <p class="SimplePara">53.41</p> </td></tr><tr><td> <p class="SimplePara">Correlation (<em class="EmphasisTypeItalic ">x</em>,Â <em class="EmphasisTypeItalic ">y</em>) [<span class="CitationRef"><a href="#CR43">43</a></span>]</p> </td><td> <p class="SimplePara">â0.33</p> </td><td> <p class="SimplePara">89.87</p> </td><td> <p class="SimplePara">77.16</p> </td></tr><tr><td> <p class="SimplePara">Cross-correlation (<em class="EmphasisTypeItalic ">x</em>,Â <em class="EmphasisTypeItalic ">y</em>) [<span class="CitationRef"><a href="#CR5">5</a></span>]</p> </td><td> <p class="SimplePara">450</p> </td><td> <p class="SimplePara">81.25</p> </td><td> <p class="SimplePara">71.27</p> </td></tr><tr><td> <p class="SimplePara">Differences [<span class="CitationRef"><a href="#CR55">55</a></span>]</p> </td><td> <p class="SimplePara">6</p> </td><td> <p class="SimplePara">100.00</p> </td><td> <p class="SimplePara">99.63</p> </td></tr><tr><td> <p class="SimplePara">Zero-crossings [<span class="CitationRef"><a href="#CR11">11</a></span>]</p> </td><td> <p class="SimplePara">36.6</p> </td><td> <p class="SimplePara">75.00</p> </td><td> <p class="SimplePara">75.70</p> </td></tr><tr><td> <p class="SimplePara">SMA [<span class="CitationRef"><a href="#CR21">21</a></span>]</p> </td><td> <p class="SimplePara">402</p> </td><td> <p class="SimplePara">79.09</p> </td><td> <p class="SimplePara">53.78</p> </td></tr><tr><td> <p class="SimplePara">SVM [<span class="CitationRef"><a href="#CR21">21</a></span>]</p> </td><td> <p class="SimplePara">228.9</p> </td><td> <p class="SimplePara">74.14</p> </td><td> <p class="SimplePara">70.72</p> </td></tr><tr><td> <p class="SimplePara">DSVM [<span class="CitationRef"><a href="#CR19">19</a></span>]</p> </td><td> <p class="SimplePara">58,280</p> </td><td> <p class="SimplePara">75.00</p> </td><td> <p class="SimplePara">72.74</p> </td></tr></tbody></table></div></div> </div></section><section id="Sec35" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">3.2.2 </span>Results for frequency-domain metrics</h4><div class="Para">TableÂ <span class="InternalRef"><a href="#Tab13">13</a></span> presents the results for the frequency-domain metrics. Overall, the results for the three-activity scenario are unsatisfying, as the increased complexity does not translate on an improvement in accuracy. For both the <em class="EmphasisTypeItalic ">coefficient sum</em> metric and the <em class="EmphasisTypeItalic ">dominant frequency</em> metric, the accuracy for the test set is actually higher than the accuracy for the training set. This can be explained by the fact that the metric results on the training set are probably more dispersed than in the test set. During the training phase, when the thresholds are chosen, it may not be possible to accommodate some points of the training set; however, the points in the test set are closer together, which means that they can be classified with fewer errors.<div id="Tab13" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">TableÂ 13</span> <p class="SimplePara">Summary of the results obtained by applying the frequency-domain metrics to the collected data the three activities of <em class="EmphasisTypeItalic ">walking, running</em> and <em class="EmphasisTypeItalic ">jumping</em> </p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1" /><col class="tcol2 align-left" /><col class="tcol3 align-char" /><col class="tcol4 align-char" /><col class="tcol5 align-char" /></colgroup><thead><tr><th rowspan="2"> <p class="SimplePara">Frequency-domain metric</p> </th><th colspan="2"> <p class="SimplePara">Parameter values</p> </th><th rowspan="2"> <p class="SimplePara">Training accuracy (%)</p> </th><th rowspan="2"> <p class="SimplePara">Test accuracy (%)</p> </th></tr><tr><th> <p class="SimplePara"> <em class="EmphasisTypeItalic ">thr</em> <sub>1</sub> </p> </th><th> <p class="SimplePara"> <em class="EmphasisTypeItalic ">thr</em> <sub>2</sub> </p> </th></tr></thead><tbody><tr><td> <p class="SimplePara">Energy [<span class="CitationRef"><a href="#CR5">5</a></span>]</p> </td><td> <p class="SimplePara">511</p> </td><td> <p class="SimplePara">987</p> </td><td> <p class="SimplePara">81.28</p> </td><td> <p class="SimplePara">76.97</p> </td></tr><tr><td> <p class="SimplePara">Entropy [<span class="CitationRef"><a href="#CR5">5</a></span>]</p> </td><td> <p class="SimplePara">6.94</p> </td><td> <p class="SimplePara">6.93</p> </td><td> <p class="SimplePara">67.45</p> </td><td> <p class="SimplePara">65.23</p> </td></tr><tr><td> <p class="SimplePara">Coeff. sum [<span class="CitationRef"><a href="#CR62">62</a></span>] (<em class="EmphasisTypeItalic ">F</em> <sub> <em class="EmphasisTypeItalic ">i</em> </sub>Â =Â 0.7Â Hz,Â <em class="EmphasisTypeItalic ">F</em> <sub> <em class="EmphasisTypeItalic ">f</em> </sub>Â =Â 3Â Hz)</p> </td><td> <p class="SimplePara">2,420</p> </td><td> <p class="SimplePara">6,070</p> </td><td> <p class="SimplePara">77.07</p> </td><td> <p class="SimplePara">79.90</p> </td></tr><tr><td> <p class="SimplePara">Dominant freq. [<span class="CitationRef"><a href="#CR23">23</a></span>] </p> </td><td> <p class="SimplePara">2.5Â Hz</p> </td><td> <p class="SimplePara">2.8Â Hz</p> </td><td> <p class="SimplePara">52.78</p> </td><td> <p class="SimplePara">58.25</p> </td></tr><tr><td> <p class="SimplePara">Wavelet ((<em class="EmphasisTypeItalic ">H</em> <sub>2</sub> and coeff. sum) [<span class="CitationRef"><a href="#CR34">34</a></span>]</p> </td><td> <p class="SimplePara">22,950</p> </td><td> <p class="SimplePara">30,730</p> </td><td> <p class="SimplePara">68.47</p> </td><td> <p class="SimplePara">41.60</p> </td></tr></tbody></table></div></div> </div><p class="Para">On the other hand, techniques that display similar results in the training set and in the test set, even if their accuracy is not too high, are more easily adaptable to signal variations within the same activity. These variations may be due to different users, different kinds of clothing, or even different ways of walking, running and jumping. So it is important to look for those techniques that are more consistent across the training set and the test set, besides looking for the best accuracy.</p><p class="Para">Frequency-domain techniques seem to provide both fairly good accuracy (except perhaps for the <em class="EmphasisTypeItalic ">dominant frequency</em> metric) and also fairly good consistency. The average accuracy for the training set is 69.6% and for the test set is 70.1%, the best accuracy being achieved by the <em class="EmphasisTypeItalic ">coeff. sum</em> metric. This provides an overall idea of the typical accuracy that can be obtained with frequency-domain metrics in a three-activity scenario.</p><div class="Para">Similar to what had happened in the case of time-domain metrics, the results for the frequency-domain metrics are better in the two-activity scenario, as shown in TableÂ <span class="InternalRef"><a href="#Tab14">14</a></span>. The average accuracy now increases to 79.5% for the training set and 78.0% for the test set. The most noticeable improvement is in the <em class="EmphasisTypeItalic ">entropy</em> metric, while the others provide smaller improvements to what had been obtained in the three-activity scenario.<div id="Tab14" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">TableÂ 14</span> <p class="SimplePara">Summary of the results obtained by applying the frequency-domain metrics to the collected data for the two activities of <em class="EmphasisTypeItalic ">walking</em> and <em class="EmphasisTypeItalic ">running</em> </p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1" /><col class="tcol2 align-left" /><col class="tcol3 align-char" /><col class="tcol4 align-char" /></colgroup><thead><tr><th rowspan="2"> <p class="SimplePara">Frequency-domain metric</p> </th><th> <p class="SimplePara">Parameter value</p> </th><th rowspan="2"> <p class="SimplePara">Training accuracy (%)</p> </th><th rowspan="2"> <p class="SimplePara">Test accuracy (%)</p> </th></tr><tr><th> <p class="SimplePara"> <em class="EmphasisTypeItalic ">thr</em> <sub>1</sub> </p> </th></tr></thead><tbody><tr><td> <p class="SimplePara">Energy [<span class="CitationRef"><a href="#CR5">5</a></span>]</p> </td><td> <p class="SimplePara">589</p> </td><td> <p class="SimplePara">83.19</p> </td><td> <p class="SimplePara">81.79</p> </td></tr><tr><td> <p class="SimplePara">Entropy [<span class="CitationRef"><a href="#CR5">5</a></span>]</p> </td><td> <p class="SimplePara">6.92</p> </td><td> <p class="SimplePara">85.56</p> </td><td> <p class="SimplePara">81.58</p> </td></tr><tr><td> <p class="SimplePara">Coeff. sum [<span class="CitationRef"><a href="#CR62">62</a></span>] (<em class="EmphasisTypeItalic ">F</em> <sub> <em class="EmphasisTypeItalic ">i</em> </sub>Â =Â 0.7Â Hz,Â <em class="EmphasisTypeItalic ">F</em> <sub> <em class="EmphasisTypeItalic ">f</em> </sub>Â =Â 3Â Hz)</p> </td><td> <p class="SimplePara">2430</p> </td><td> <p class="SimplePara">82.54</p> </td><td> <p class="SimplePara">79.01</p> </td></tr><tr><td> <p class="SimplePara">Dominant freq. [<span class="CitationRef"><a href="#CR23">23</a></span>]</p> </td><td> <p class="SimplePara">2.6Â Hz</p> </td><td> <p class="SimplePara">66.81</p> </td><td> <p class="SimplePara">69.61</p> </td></tr><tr><td> <p class="SimplePara">Wavelet (coeff. sum) [<span class="CitationRef"><a href="#CR34">34</a></span>]</p> </td><td> <p class="SimplePara">22,950</p> </td><td> <p class="SimplePara">75.86</p> </td><td> <p class="SimplePara">52.85</p> </td></tr></tbody></table></div></div> </div></section><section id="Sec36" tabindex="-1" class="Section3 RenderAsSection3"><h4 class="Heading"><span class="HeadingNumber">3.2.3 </span>Results for symbolic string-domain metrics</h4><p class="Para">We now present results for the use of some metrics using a string representation of the raw accelerometer signal. In these experiments, we used the SAX representation [<span class="CitationRef"><a href="#CR31">31</a></span>] for windows with length of 512 samples. We varied the size of the alphabet and the number of consecutive raw samples that are represent by one string symbol. One file for each activity was selected as the reference sample for that activity. All other samples were then tested against that representative sample to evaluate the accuracy of the recognition.</p><div class="Para">The results for the three-activity scenario using this string representation and the set of metrics depicted in TableÂ <span class="InternalRef"><a href="#Tab15">15</a></span> are not very encouraging. The overall accuracy rate for the training set and the test set is 62.4 and 45.1% respectively. Even ignoring the <em class="EmphasisTypeItalic ">minimum distance</em> metric that achieves a level of accuracy that is comparable to a random selection of the activities, only the <em class="EmphasisTypeItalic ">Levenshtein</em> metric achieves a reasonable result, outperforming the more sophisticated <em class="EmphasisTypeItalic ">DTW</em> metric.<div id="Tab15" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">TableÂ 15</span> <p class="SimplePara">Summary of the results obtained by applying symbolic string-domain metrics to collected data for the three activities <em class="EmphasisTypeItalic ">walking, running</em> and <em class="EmphasisTypeItalic ">jumping</em> </p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1" /><col class="tcol2 align-char" /><col class="tcol3 align-char" /><col class="tcol4 align-char" /><col class="tcol5 align-char" /></colgroup><thead><tr><th rowspan="2"> <p class="SimplePara">String-domain metric</p> </th><th colspan="2"> <p class="SimplePara">Parameter values</p> </th><th rowspan="2"> <p class="SimplePara">Training accuracy (%)</p> </th><th rowspan="2"> <p class="SimplePara">Test accuracy (%)</p> </th></tr><tr><th> <p class="SimplePara">Alphabet</p> </th><th> <p class="SimplePara">Window</p> </th></tr></thead><tbody><tr><td> <p class="SimplePara">Minimum distance [<span class="CitationRef"><a href="#CR32">32</a></span>]</p> </td><td> <p class="SimplePara">3</p> </td><td> <p class="SimplePara">6</p> </td><td> <p class="SimplePara">50.00</p> </td><td> <p class="SimplePara">32.03</p> </td></tr><tr><td> <p class="SimplePara">Levenshtein [<span class="CitationRef"><a href="#CR14">14</a></span>]</p> </td><td> <p class="SimplePara">9</p> </td><td> <p class="SimplePara">9</p> </td><td> <p class="SimplePara">76.80</p> </td><td> <p class="SimplePara">52.50</p> </td></tr><tr><td> <p class="SimplePara">DTW [<span class="CitationRef"><a href="#CR46">46</a></span>]</p> </td><td> <p class="SimplePara">8</p> </td><td> <p class="SimplePara">11</p> </td><td> <p class="SimplePara">60.30</p> </td><td> <p class="SimplePara">50.50</p> </td></tr></tbody></table></div></div> </div><div class="Para">In contrast, the results for the recognition of two activities as depicted in TableÂ <span class="InternalRef"><a href="#Tab16">16</a></span> are significantly better with an overall accuracy rate for the training set and the test set of 73.4 and 77.5%, respectively. Not only the <em class="EmphasisTypeItalic ">Levenshtein</em> and the <em class="EmphasisTypeItalic ">DTW</em> metrics present much better accuracy than their use in the three-activity scenario, but the gap between training set and test set is very small, which is a good indicator of the adaptability of these techniques in scenarios where only a binary classification is needed.<div id="Tab16" class="Table"><div class="Caption" lang="en"><div class="CaptionContent"><span class="CaptionNumber">TableÂ 16</span> <p class="SimplePara">Summary of the results obtained by applying symbolic string-domain metrics to collected data for the two activities of <em class="EmphasisTypeItalic ">walking</em> and <em class="EmphasisTypeItalic ">running</em> </p> </div></div><div class="u-scroll-horizontal"><table><colgroup><col class="tcol1" /><col class="tcol2 align-char" /><col class="tcol3 align-char" /><col class="tcol4 align-char" /><col class="tcol5 align-char" /></colgroup><thead><tr><th rowspan="2"> <p class="SimplePara">String-Domain Metric</p> </th><th colspan="2"> <p class="SimplePara">Parameter values</p> </th><th rowspan="2"> <p class="SimplePara">Training accuracy (%)</p> </th><th rowspan="2"> <p class="SimplePara">Test accuracy (%)</p> </th></tr><tr><th> <p class="SimplePara">Alphabet</p> </th><th> <p class="SimplePara">Window</p> </th></tr></thead><tbody><tr><td> <p class="SimplePara">Minimum distance [<span class="CitationRef"><a href="#CR32">32</a></span>]</p> </td><td> <p class="SimplePara">6</p> </td><td> <p class="SimplePara">6</p> </td><td> <p class="SimplePara">58.47</p> </td><td> <p class="SimplePara">67.63</p> </td></tr><tr><td> <p class="SimplePara">Levenshtein [<span class="CitationRef"><a href="#CR14">14</a></span>]</p> </td><td> <p class="SimplePara">3</p> </td><td> <p class="SimplePara">6</p> </td><td> <p class="SimplePara">81.36</p> </td><td> <p class="SimplePara">80.58</p> </td></tr><tr><td> <p class="SimplePara">DTW [<span class="CitationRef"><a href="#CR46">46</a></span>]</p> </td><td> <p class="SimplePara">7</p> </td><td> <p class="SimplePara">10</p> </td><td> <p class="SimplePara">80.51</p> </td><td> <p class="SimplePara">84.17</p> </td></tr></tbody></table></div></div> </div></section></section><section id="Sec37" tabindex="-1" class="Section2 RenderAsSection2 SectionTypeDiscussion"><h3 class="Heading"><span class="HeadingNumber">3.3 </span>Discussion</h3><p class="Para">Overall, the results for the classification of the three physical activities are not outstanding. Frequency-domain metrics in particular were expected to perform better, given the computational cost of computing the frequency spectrum either using FFT-based methods or using Wavelets. Fortunately, the results are significantly better in a two-activity scenario. Somewhat surprising is the good performance of some of the simpler time-based metrics making them a method of choice for embedded mobile devices where energy and storage is at premium. String-based metrics exhibit good accuracy results only in the two-activity scenario. Given their simpler computational costs these methods also have the potential for better results if further preprocessing is applied, and they offer the added benefit of a natural compression feature and discrete representation.</p></section></div></section><section id="Sec38" tabindex="-1" class="Section1 RenderAsSection1"><h2 class="Heading"><span class="HeadingNumber">4 </span>Conclusion</h2><div class="content"><p class="Para">In this article, we have presented a survey of the techniques for recognizing daily physical activities such as <em class="EmphasisTypeItalic ">walking</em> from raw accelerometer data. We described techniques that operate on the time domain and on the frequency domain, and also on symbolic data representations that can be used to discriminate between user activities. We evaluated a representative set of techniques from these domains on a sample data obtained in a real-world environment for the activities of <em class="EmphasisTypeItalic ">walking, running</em> and <em class="EmphasisTypeItalic ">jumping</em>. The evaluation focused on the ability of the techniques to successfully distinguish those user activities in a three-activity and in a two-activity scenario. The results reveal that for the three-activity scenario some of the frequency-domain techniques are particularly robust and have comparable performance to selected time-domain techniques. For the two-activity scenario, however, the best time-domain techniques outperform the best frequency-domain techniques. Regarding the symbolic-domain techniques, their performance is only acceptable for the two-activity scenario, making them attractive if their simpler implementation and their potential for data compression is taken into account, Overall, we hope this assessment will contribute to identify the preprocessing techniques that are better suited for accelerometer-based and context inference applications.</p></div></section><section id="Footnotes" class="FootnoteSection Section1 RenderAsSection1"><h2 class="Heading">Footnotes</h2><div class="content"><ol><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn1_source">1</a>.</span><div class="FootnoteContent" id="Fn1"><p class="Para">For example, the Nike+ project (<span class="ExternalRef"><a target="_blank" rel="noopener" href="http://www.nikerunning.nike.com/nikeplus/"><span class="RefSource">http://www.nikerunning.nike.com/nikeplus/</span></a></span>) collects data captured by an accelerometer located on the userâs running shoes. The user can then upload the data to a personal computer and use an application that analyzes the running habits and physical effort to recommend training regimes.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn2_source">2</a>.</span><div class="FootnoteContent" id="Fn2"><p class="Para">Characteristics such as clock rates, caches, functional units and pipelines could influence the results of such comparison, as some techniques may be more amenable to specific architectural or compiler features.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn3_source">3</a>.</span><div class="FootnoteContent" id="Fn3"><p class="Para">In the simplest Wavelet examined, the Haar wavelet of order 2 (<em class="EmphasisTypeItalic ">H</em> <sub>2</sub>Â =Â [11; 1Â âÂ 1]) only additions and subtractions are used and divisions are always by constant, which is optimized in many Floating-Point Units (FPU) hardware designs.</p></div></li><li class="Footnote"><span class="FootnoteNumber"><a href="#Fn4_source">4</a>.</span><div class="FootnoteContent" id="Fn4"><p class="Para">Although clever implementations can reduce this requirement to a linear relationship.</p></div></li></ol></div></section></div><section class="Section1 RenderAsSection1" id="Bib1" tabindex="-1"><h2 class="Heading">References</h2><div class="content"><ol class="BibliographyWrapper"><li class="Citation"><div class="CitationNumber">1.</div><div class="CitationContent" id="CR1">Al-ani T, Ba QTL, Monacelli E (2006) On-line automatic detection of human activity in home using wavelet and hidden markov models scilab toolkits. In: Proceedings of the 16th international conference on control applications (ICCA)<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Al-ani%20T%2C%20Ba%20QTL%2C%20Monacelli%20E%20%282006%29%20On-line%20automatic%20detection%20of%20human%20activity%20in%20home%20using%20wavelet%20and%20hidden%20markov%20models%20scilab%20toolkits.%20In%3A%20Proceedings%20of%20the%2016th%20international%20conference%20on%20control%20applications%20%28ICCA%29"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">2.</div><div class="CitationContent" id="CR2">Aminian K, Robert P, Jequier E, Schutz Y (1995) Estimation of speed and incline of walking using neural network. IEEE Trans Instrum Meas 44(3):743â746<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/19.387322"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Estimation%20of%20speed%20and%20incline%20of%20walking%20using%20neural%20network&amp;author=K.%20Aminian&amp;author=P.%20Robert&amp;author=E.%20Jequier&amp;author=Y.%20Schutz&amp;journal=IEEE%20Trans%20Instrum%20Meas&amp;volume=44&amp;issue=3&amp;pages=743-746&amp;publication_year=1995"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">3.</div><div class="CitationContent" id="CR3">Aminian K, Robert P, Buchser E, Rutschmann B, Hayoz D, Depairon M (1999) Physical activity monitoring based on accelerometry: validation and comparison with video observation. Med Biol Eng Comput 37(1):304â308<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1007/BF02513304"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Physical%20activity%20monitoring%20based%20on%20accelerometry%3A%20validation%20and%20comparison%20with%20video%20observation&amp;author=K.%20Aminian&amp;author=P.%20Robert&amp;author=E.%20Buchser&amp;author=B.%20Rutschmann&amp;author=D.%20Hayoz&amp;author=M.%20Depairon&amp;journal=Med%20Biol%20Eng%20Comput&amp;volume=37&amp;issue=1&amp;pages=304-308&amp;publication_year=1999"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">4.</div><div class="CitationContent" id="CR4">Ashbrook D (1999) Context sensing with the twiddler keyboard. In: Proceedings of the third international symposium on wearable computers (ISWCâ99), pp 197â198<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Ashbrook%20D%20%281999%29%20Context%20sensing%20with%20the%20twiddler%20keyboard.%20In%3A%20Proceedings%20of%20the%20third%20international%20symposium%20on%20wearable%20computers%20%28ISWC%E2%80%9999%29%2C%20pp%20197%E2%80%93198"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">5.</div><div class="CitationContent" id="CR5">Bao L, Intille SS (2004) Activity recognition from user-annotated acceleration data. In: Proceedings of the interantional conference on pervasive computing (PERVASIVEâ04). Springer, Lecture notes on computer sciences (LNCS), vol 3001, pp 1â17<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Bao%20L%2C%20Intille%20SS%20%282004%29%20Activity%20recognition%20from%20user-annotated%20acceleration%20data.%20In%3A%20Proceedings%20of%20the%20interantional%20conference%20on%20pervasive%20computing%20%28PERVASIVE%E2%80%9904%29.%20Springer%2C%20Lecture%20notes%20on%20computer%20sciences%20%28LNCS%29%2C%20vol%203001%2C%20pp%201%E2%80%9317"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">6.</div><div class="CitationContent" id="CR6">Bouten C, Koekkoek K, Verduin M, Kodde R, Janssen J (1997) A triaxial accelerometer and portable data processing unit for the assessment of daily physical activity. IEEE Trans Biomed Eng 44(3):136â147<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/10.554760"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=A%20triaxial%20accelerometer%20and%20portable%20data%20processing%20unit%20for%20the%20assessment%20of%20daily%20physical%20activity&amp;author=C.%20Bouten&amp;author=K.%20Koekkoek&amp;author=M.%20Verduin&amp;author=R.%20Kodde&amp;author=J.%20Janssen&amp;journal=IEEE%20Trans%20Biomed%20Eng&amp;volume=44&amp;issue=3&amp;pages=136-147&amp;publication_year=1997"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">7.</div><div class="CitationContent" id="CR7">Brown G, Bajcsy R, Eklund M (2005) An accelerometer based fall detector: development, experimentation, and analysis. Technical Report 12, University of California at Berkeley<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Brown%20G%2C%20Bajcsy%20R%2C%20Eklund%20M%20%282005%29%20An%20accelerometer%20based%20fall%20detector%3A%20development%2C%20experimentation%2C%20and%20analysis.%20Technical%20Report%2012%2C%20University%20of%20California%20at%20Berkeley"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">8.</div><div class="CitationContent" id="CR8">Chambers G, Venkatesh S, West G, Bui H (2002) Hierarchical recognition of intentional human gestures for sports video annotation. In: Proceedings of the 16th international conference on pattern recognition, vol 2, pp 1082â1085<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Chambers%20G%2C%20Venkatesh%20S%2C%20West%20G%2C%20Bui%20H%20%282002%29%20Hierarchical%20recognition%20of%20intentional%20human%20gestures%20for%20sports%20video%20annotation.%20In%3A%20Proceedings%20of%20the%2016th%20international%20conference%20on%20pattern%20recognition%2C%20vol%202%2C%20pp%201082%E2%80%931085"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">9.</div><div class="CitationContent" id="CR9">Chen J, Kwong K, Chang D, Luk J, Bajcs R (2005) Wearable sensors for reliable fall detection. In: Proceedings of the 27th annual IEEE conference on engineering in medicine and biology (EMBâ05)<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Chen%20J%2C%20Kwong%20K%2C%20Chang%20D%2C%20Luk%20J%2C%20Bajcs%20R%20%282005%29%20Wearable%20sensors%20for%20reliable%20fall%20detection.%20In%3A%20Proceedings%20of%20the%2027th%20annual%20IEEE%20conference%20on%20engineering%20in%20medicine%20and%20biology%20%28EMB%E2%80%9905%29"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">10.</div><div class="CitationContent" id="CR10">Dargie W (2006) A distributed architecture for computing context in mobile devices. Masterâs thesis, Dresden University of Technology, Department of Computer Science<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Dargie%20W%20%282006%29%20A%20distributed%20architecture%20for%20computing%20context%20in%20mobile%20devices.%20Master%E2%80%99s%20thesis%2C%20Dresden%20University%20of%20Technology%2C%20Department%20of%20Computer%20Science"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">11.</div><div class="CitationContent" id="CR11">Farringdon J, Moore AJ, Tilbury N, Church J, Biemond PD (1999) Wearable sensor badge and sensor jacket for context awareness. In: Proceedings of the IEEE international symposium on wearable computers (ISWCâ99). IEEE Computer Society, pp 107â114<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Farringdon%20J%2C%20Moore%20AJ%2C%20Tilbury%20N%2C%20Church%20J%2C%20Biemond%20PD%20%281999%29%20Wearable%20sensor%20badge%20and%20sensor%20jacket%20for%20context%20awareness.%20In%3A%20Proceedings%20of%20the%20IEEE%20international%20symposium%20on%20wearable%20computers%20%28ISWC%E2%80%9999%29.%20IEEE%20Computer%20Society%2C%20pp%20107%E2%80%93114"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">12.</div><div class="CitationContent" id="CR12">Golding A, Lesh N (1999) Indoor navigation using a diverse set of cheap, wearable sensors. In: Proceedings of the 3rd IEEE international symposium on wearable computers, pp 29â36<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Golding%20A%2C%20Lesh%20N%20%281999%29%20Indoor%20navigation%20using%20a%20diverse%20set%20of%20cheap%2C%20wearable%20sensors.%20In%3A%20Proceedings%20of%20the%203rd%20IEEE%20international%20symposium%20on%20wearable%20computers%2C%20pp%2029%E2%80%9336"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">13.</div><div class="CitationContent" id="CR13">Guerreiro T, Gamboa R, Jorge J (2008) Mnemonical body shortcuts: improving mobile interaction. In: Proceedings of the 15th European conference on cognitive ergonomics (ECCEâ08). ACM, pp 1â8<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Guerreiro%20T%2C%20Gamboa%20R%2C%20Jorge%20J%20%282008%29%20Mnemonical%20body%20shortcuts%3A%20improving%20mobile%20interaction.%20In%3A%20Proceedings%20of%20the%2015th%20European%20conference%20on%20cognitive%20ergonomics%20%28ECCE%E2%80%9908%29.%20ACM%2C%20pp%201%E2%80%938"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">14.</div><div class="CitationContent" id="CR14">Gusfield D (1997) Algorithms on strings, trees, and sequences. Cambridge University Press, Cambridge<span class="Occurrences"><span class="Occurrence OccurrenceZLBID"><a class="gtm-reference" data-reference-type="MATH" target="_blank" rel="noopener" href="http://www.emis.de/MATH-item?0934.68103"><span><span>zbMATH</span></span></a></span><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1017/CBO9780511574931"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Algorithms%20on%20strings%2C%20trees%2C%20and%20sequences&amp;author=D.%20Gusfield&amp;publication_year=1997"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">15.</div><div class="CitationContent" id="CR15">Healey J, Logan B (2005) Wearable wellness monitoring using ecg and accelerometer data. In: Proceedings of the ninth IEEE international symposium on wearable computers (ISWCâ05). IEEE Computer Society Press, Washington, DC, pp 220â221<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Healey%20J%2C%20Logan%20B%20%282005%29%20Wearable%20wellness%20monitoring%20using%20ecg%20and%20accelerometer%20data.%20In%3A%20Proceedings%20of%20the%20ninth%20IEEE%20international%20symposium%20on%20wearable%20computers%20%28ISWC%E2%80%9905%29.%20IEEE%20Computer%20Society%20Press%2C%20Washington%2C%20DC%2C%20pp%20220%E2%80%93221"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">16.</div><div class="CitationContent" id="CR16">Ho J (2004) Interruptions: using activity transitions to trigger proactive messages. Masterâs thesis, Massachusetts Institute of Technology<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Ho%20J%20%282004%29%20Interruptions%3A%20using%20activity%20transitions%20to%20trigger%20proactive%20messages.%20Master%E2%80%99s%20thesis%2C%20Massachusetts%20Institute%20of%20Technology"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">17.</div><div class="CitationContent" id="CR17">Huynh T, Schiele B (2005) Analyzing features for activity recognition. In: sOc-EUSAI â05: Proceedings of the 2005 joint conference on smart objects and ambient intelligence, pp 159â163<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Huynh%20T%2C%20Schiele%20B%20%282005%29%20Analyzing%20features%20for%20activity%20recognition.%20In%3A%20sOc-EUSAI%20%E2%80%9905%3A%20Proceedings%20of%20the%202005%20joint%20conference%20on%20smart%20objects%20and%20ambient%20intelligence%2C%20pp%20159%E2%80%93163"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">18.</div><div class="CitationContent" id="CR18">Intille SS, Bao L, Tapia EM, Rondoni J (2004) Acquiring in situ training data for context-aware ubiquitous computing applications. In: Proceedings of the SIGCHI conference on human factors in computing systems (CHIâ04). ACM Press, pp 1â8<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Intille%20SS%2C%20Bao%20L%2C%20Tapia%20EM%2C%20Rondoni%20J%20%282004%29%20Acquiring%20in%20situ%20training%20data%20for%20context-aware%20ubiquitous%20computing%20applications.%20In%3A%20Proceedings%20of%20the%20SIGCHI%20conference%20on%20human%20factors%20in%20computing%20systems%20%28CHI%E2%80%9904%29.%20ACM%20Press%2C%20pp%201%E2%80%938"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">19.</div><div class="CitationContent" id="CR19">Jeong DU, Kim SJ, Chung WY (2007) Classification of posture and movement using a 3-axis accelerometer. In: Proceedings of the 2007 international conference on convergence information technology (ICCITâ07). IEEE Computer Society, Washington, DC, USA, pp 837â844<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Jeong%20DU%2C%20Kim%20SJ%2C%20Chung%20WY%20%282007%29%20Classification%20of%20posture%20and%20movement%20using%20a%203-axis%20accelerometer.%20In%3A%20Proceedings%20of%20the%202007%20international%20conference%20on%20convergence%20information%20technology%20%28ICCIT%E2%80%9907%29.%20IEEE%20Computer%20Society%2C%20Washington%2C%20DC%2C%20USA%2C%20pp%20837%E2%80%93844"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">20.</div><div class="CitationContent" id="CR20">Jin G, Lee S, Lee T (2007) Context awareness of human motion states using accelerometer. J Med Syst 32(2):93â100<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1007/s10916-007-9111-y"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Context%20awareness%20of%20human%20motion%20states%20using%20accelerometer&amp;author=G.%20Jin&amp;author=S.%20Lee&amp;author=T.%20Lee&amp;journal=J%20Med%20Syst&amp;volume=32&amp;issue=2&amp;pages=93-100&amp;publication_year=2007"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">21.</div><div class="CitationContent" id="CR21">Karantonis D, Narayanan M, Mathie M, Lovell N, Celler B (2006) Implementation of a real-time human movement classifier using a triaxial accelerometer for ambulatory monitoring. IEEE Trans Inform Technol Biomed 10(1):156â167<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/TITB.2005.856864"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Implementation%20of%20a%20real-time%20human%20movement%20classifier%20using%20a%20triaxial%20accelerometer%20for%20ambulatory%20monitoring&amp;author=D.%20Karantonis&amp;author=M.%20Narayanan&amp;author=M.%20Mathie&amp;author=N.%20Lovell&amp;author=B.%20Celler&amp;journal=IEEE%20Trans%20Inform%20Technol%20Biomed&amp;volume=10&amp;issue=1&amp;pages=156-167&amp;publication_year=2006"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">22.</div><div class="CitationContent" id="CR22">Kawahara HSY, Hisashi Kurasawa HM, Aoyama T (2005) A context-aware collaborative filtering algorithm for real world oriented content delivery service. In: Proceedings of ubiPCMM<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Kawahara%20HSY%2C%20Hisashi%20Kurasawa%20HM%2C%20Aoyama%20T%20%282005%29%20A%20context-aware%20collaborative%20filtering%20algorithm%20for%20real%20world%20oriented%20content%20delivery%20service.%20In%3A%20Proceedings%20of%20ubiPCMM"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">23.</div><div class="CitationContent" id="CR23">Kawahara Y, Kurasawa H, Morikawa H (2007) Recognizing user context using mobile handsets with acceleration sensors. In: (IEEE) international conference on portable information devices (PORTABLEâ07), pp 1â5<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Kawahara%20Y%2C%20Kurasawa%20H%2C%20Morikawa%20H%20%282007%29%20Recognizing%20user%20context%20using%20mobile%20handsets%20with%20acceleration%20sensors.%20In%3A%20%28IEEE%29%20international%20conference%20on%20portable%20information%20devices%20%28PORTABLE%E2%80%9907%29%2C%20pp%201%E2%80%935"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">24.</div><div class="CitationContent" id="CR24">Kawaharaq Y, Ryu N, Asami T (2009) Monitoring daily energy expenditure using a 3-axis accelerometer with a low-power microprocessor. e-Minds: Int J Hum Comput Interact 1(5):145â154<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Monitoring%20daily%20energy%20expenditure%20using%20a%203-axis%20accelerometer%20with%20a%20low-power%20microprocessor.&amp;author=Y.%20Kawaharaq&amp;author=N.%20Ryu&amp;author=T.%20Asami&amp;journal=e-Minds%3A%20Int%20J%20Hum%20Comput%20Interact&amp;volume=1&amp;issue=5&amp;pages=145-154&amp;publication_year=2009"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">25.</div><div class="CitationContent" id="CR25">Keogh E, Pazzani M (2001) Derivative dynamic time warping. In: Proceedings of the first SIAM international conference on data mining (SDMÃ2001)<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Keogh%20E%2C%20Pazzani%20M%20%282001%29%20Derivative%20dynamic%20time%20warping.%20In%3A%20Proceedings%20of%20the%20first%20SIAM%20international%20conference%20on%20data%20mining%20%28SDM%C3%952001%29"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">26.</div><div class="CitationContent" id="CR26">Keogh E, Lonardi S, Ratanamahatana CA, Wei L, Lee SH, Handley J (2007) Compression-based data mining of sequential data. Data Min Knowl Discov 14(1):99â129<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1007/s10618-006-0049-3"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceAMSID"><a class="gtm-reference" data-reference-type="MathSciNet" target="_blank" rel="noopener" href="http://www.ams.org/mathscinet-getitem?mr=2359174"><span><span>MathSciNet</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Compression-based%20data%20mining%20of%20sequential%20data&amp;author=E.%20Keogh&amp;author=S.%20Lonardi&amp;author=CA.%20Ratanamahatana&amp;author=L.%20Wei&amp;author=SH.%20Lee&amp;author=J.%20Handley&amp;journal=Data%20Min%20Knowl%20Discov&amp;volume=14&amp;issue=1&amp;pages=99-129&amp;publication_year=2007"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">27.</div><div class="CitationContent" id="CR27">Kern N, Schiele B, Schmidt A (2007) Recognizing context for annotating a live life recording. Pers Ubiquit Comput 11(4):251â263<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1007/s00779-006-0086-3"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Recognizing%20context%20for%20annotating%20a%20live%20life%20recording&amp;author=N.%20Kern&amp;author=B.%20Schiele&amp;author=A.%20Schmidt&amp;journal=Pers%20Ubiquit%20Comput&amp;volume=11&amp;issue=4&amp;pages=251-263&amp;publication_year=2007"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">28.</div><div class="CitationContent" id="CR28">Kim IJ, Im S, Hong E, Ahn SC, Kim HG (2007) ADL classification using triaxial accelerometers and RFID. In: International workshop on ubiquitous convergence technology (IWUCTâ07)<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Kim%20IJ%2C%20Im%20S%2C%20Hong%20E%2C%20Ahn%20SC%2C%20Kim%20HG%20%282007%29%20ADL%20classification%20using%20triaxial%20accelerometers%20and%20RFID.%20In%3A%20International%20workshop%20on%20ubiquitous%20convergence%20technology%20%28IWUCT%E2%80%9907%29"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">29.</div><div class="CitationContent" id="CR29">Krause A, Siewiorek D, Smailagic A, Farringdon J (2003) Unsupervised, dynamic identification of physiological and activity context in wearable computing. In: Proceedings of seventh IEEE international symposium on wearable computers (ISWCâ03), pp 88â97<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Krause%20A%2C%20Siewiorek%20D%2C%20Smailagic%20A%2C%20Farringdon%20J%20%282003%29%20Unsupervised%2C%20dynamic%20identification%20of%20physiological%20and%20activity%20context%20in%20wearable%20computing.%20In%3A%20Proceedings%20of%20seventh%20IEEE%20international%20symposium%20on%20wearable%20computers%20%28ISWC%E2%80%9903%29%2C%20pp%2088%E2%80%9397"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">30.</div><div class="CitationContent" id="CR30">Lee SW, Mase K (2002) Activity and location recognition using wearable sensors. IEEE Pervasive Comput 1(3):24â32<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/MPRV.2002.1037719"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Activity%20and%20location%20recognition%20using%20wearable%20sensors&amp;author=SW.%20Lee&amp;author=K.%20Mase&amp;journal=IEEE%20Pervasive%20Comput&amp;volume=1&amp;issue=3&amp;pages=24-32&amp;publication_year=2002"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">31.</div><div class="CitationContent" id="CR31">Lin J, Keogh E, Lonardi S, Chiu B (2003) A symbolic representation of time series, with implications for streaming algorithms. In: Proceedings of the 8th ACM SIGMOD workshop on research issues in data mining and knowledge discovery<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Lin%20J%2C%20Keogh%20E%2C%20Lonardi%20S%2C%20Chiu%20B%20%282003%29%20A%20symbolic%20representation%20of%20time%20series%2C%20with%20implications%20for%20streaming%20algorithms.%20In%3A%20Proceedings%20of%20the%208th%20ACM%20SIGMOD%20workshop%20on%20research%20issues%20in%20data%20mining%20and%20knowledge%20discovery"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">32.</div><div class="CitationContent" id="CR32">Lin J, Keogh E, Wei L, Lonardi S (2007) Experiencing SAX: a novel symbolic representation of time series. Data Min Knowl Discover 15(2):107â144<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1007/s10618-007-0064-z"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceAMSID"><a class="gtm-reference" data-reference-type="MathSciNet" target="_blank" rel="noopener" href="http://www.ams.org/mathscinet-getitem?mr=2409783"><span><span>MathSciNet</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Experiencing%20SAX%3A%20a%20novel%20symbolic%20representation%20of%20time%20series&amp;author=J.%20Lin&amp;author=E.%20Keogh&amp;author=L.%20Wei&amp;author=S.%20Lonardi&amp;journal=Data%20Min%20Knowl%20Discover&amp;volume=15&amp;issue=2&amp;pages=107-144&amp;publication_year=2007"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">33.</div><div class="CitationContent" id="CR33">Liu J, Wang Z, Zhong L, Wickramasuriya J, Vasudevan V (2008) uWave: accelerometer-based personalized gesture recognition. TR0630-08, Rice University and Motorola Labs, Houston, Texas<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Liu%20J%2C%20Wang%20Z%2C%20Zhong%20L%2C%20Wickramasuriya%20J%2C%20Vasudevan%20V%20%282008%29%20uWave%3A%20accelerometer-based%20personalized%20gesture%20recognition.%20TR0630-08%2C%20Rice%20University%20and%20Motorola%20Labs%2C%20Houston%2C%20Texas"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">34.</div><div class="CitationContent" id="CR34">Mallat S (1999) A wavelet tour of signal processing. Academic Press, Berlin<span class="Occurrences"><span class="Occurrence OccurrenceZLBID"><a class="gtm-reference" data-reference-type="MATH" target="_blank" rel="noopener" href="http://www.emis.de/MATH-item?0998.94510"><span><span>zbMATH</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=A%20wavelet%20tour%20of%20signal%20processing&amp;author=S.%20Mallat&amp;publication_year=1999"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">35.</div><div class="CitationContent" id="CR35">MÃ¤ntyjÃ¤rvi J (2003) Sensor-based context recognition for mobile applications. Ph.D. thesis, University of Oulu, Finland, Faculty of Technology, Department of Electrical and Information Engineering, Information Processing Laboratory<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=M%C3%A4ntyj%C3%A4rvi%20J%20%282003%29%20Sensor-based%20context%20recognition%20for%20mobile%20applications.%20Ph.D.%20thesis%2C%20University%20of%20Oulu%2C%20Finland%2C%20Faculty%20of%20Technology%2C%20Department%20of%20Electrical%20and%20Information%20Engineering%2C%20Information%20Processing%20Laboratory"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">36.</div><div class="CitationContent" id="CR36">Martens W (1992) The Fast Time Frequency Transform (F.T.F.T.): a novel on-line approach to the instantaneous spectrum. In: Engineering in Medicine and Biology Society, vol 14. Proceedings of the annual international conference of the IEEE, vol 6, pp 2594â2595<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Martens%20W%20%281992%29%20The%20Fast%20Time%20Frequency%20Transform%20%28F.T.F.T.%29%3A%20a%20novel%20on-line%20approach%20to%20the%20instantaneous%20spectrum.%20In%3A%20Engineering%20in%20Medicine%20and%20Biology%20Society%2C%20vol%2014.%20Proceedings%20of%20the%20annual%20international%20conference%20of%20the%20IEEE%2C%20vol%206%2C%20pp%202594%E2%80%932595"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">37.</div><div class="CitationContent" id="CR37">Mathie M (2003) Monitoring and interpreting human movement patterns using a triaxial accelerometer. Ph.D. thesis, University of New South Wales<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Mathie%20M%20%282003%29%20Monitoring%20and%20interpreting%20human%20movement%20patterns%20using%20a%20triaxial%20accelerometer.%20Ph.D.%20thesis%2C%20University%20of%20New%20South%20Wales"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">38.</div><div class="CitationContent" id="CR38">Mathie M, Celler B, Lovell N, Coster A (2004) Classification of basic daily movements using a triaxial accelerometer. Med Biol Eng Comput 42(5):679â687<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1007/BF02347551"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Classification%20of%20basic%20daily%20movements%20using%20a%20triaxial%20accelerometer&amp;author=M.%20Mathie&amp;author=B.%20Celler&amp;author=N.%20Lovell&amp;author=A.%20Coster&amp;journal=Med%20Biol%20Eng%20Comput&amp;volume=42&amp;issue=5&amp;pages=679-687&amp;publication_year=2004"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">39.</div><div class="CitationContent" id="CR39">Muscillo R, Conforto S, Schmid M, Caselli P, DâAlessio T (2007) Classification of motor activities through derivative dynamic time warping applied on accelerometer data. In: Proceedings of the 29th IEEE annual international conference on Engineering in Medicine and Biology Society (EMBSâ07), pp 4930â4933<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Muscillo%20R%2C%20Conforto%20S%2C%20Schmid%20M%2C%20Caselli%20P%2C%20D%E2%80%99Alessio%20T%20%282007%29%20Classification%20of%20motor%20activities%20through%20derivative%20dynamic%20time%20warping%20applied%20on%20accelerometer%20data.%20In%3A%20Proceedings%20of%20the%2029th%20IEEE%20annual%20international%20conference%20on%20Engineering%20in%20Medicine%20and%20Biology%20Society%20%28EMBS%E2%80%9907%29%2C%20pp%204930%E2%80%934933"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">40.</div><div class="CitationContent" id="CR40">Nambu M (2007) Body surface mounted biomedical monitoring system using bluetooth. In: Proceedings of the 29th annual international conference of the IEEE on Engineering in Medicine and Biology Society, (EMBSâ07), pp 1824â1825<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Nambu%20M%20%282007%29%20Body%20surface%20mounted%20biomedical%20monitoring%20system%20using%20bluetooth.%20In%3A%20Proceedings%20of%20the%2029th%20annual%20international%20conference%20of%20the%20IEEE%20on%20Engineering%20in%20Medicine%20and%20Biology%20Society%2C%20%28EMBS%E2%80%9907%29%2C%20pp%201824%E2%80%931825"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">41.</div><div class="CitationContent" id="CR41">Nham B, Siangliulue K, Yeung S (2008) Predicting mode of transport from iphone accelerometer data. CS 229: Machine Learning Final Projects, Stanford University, Stanford, California<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Nham%20B%2C%20Siangliulue%20K%2C%20Yeung%20S%20%282008%29%20Predicting%20mode%20of%20transport%20from%20iphone%20accelerometer%20data.%20CS%20229%3A%20Machine%20Learning%20Final%20Projects%2C%20Stanford%20University%2C%20Stanford%2C%20California"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">42.</div><div class="CitationContent" id="CR42">Randell C, Muller H (2000) Context awareness by analyzing accelerometer data. In: Proceedings of the 4th IEEE international symposium on wearable computers (ISWCâ00). IEEE Computer Society, Washington, DC, USA, pp 175â176<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Randell%20C%2C%20Muller%20H%20%282000%29%20Context%20awareness%20by%20analyzing%20accelerometer%20data.%20In%3A%20Proceedings%20of%20the%204th%20IEEE%20international%20symposium%20on%20wearable%20computers%20%28ISWC%E2%80%9900%29.%20IEEE%20Computer%20Society%2C%20Washington%2C%20DC%2C%20USA%2C%20pp%20175%E2%80%93176"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">43.</div><div class="CitationContent" id="CR43">Ravi N, Dandekar N, Mysore P, Littman ML (2005) Activity recognition from accelerometer data. In: IAAIâ05: Proceedings of the 17th conference on innovative applications of artificial intelligence. AAAI Press, pp 1541â1546<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Ravi%20N%2C%20Dandekar%20N%2C%20Mysore%20P%2C%20Littman%20ML%20%282005%29%20Activity%20recognition%20from%20accelerometer%20data.%20In%3A%20IAAI%E2%80%9905%3A%20Proceedings%20of%20the%2017th%20conference%20on%20innovative%20applications%20of%20artificial%20intelligence.%20AAAI%20Press%2C%20pp%201541%E2%80%931546"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">44.</div><div class="CitationContent" id="CR44">Robert B, White B, Renter D, Larson R (2009) Evaluation of three-dimensional accelerometers to monitor and classify behavior patterns in cattle. Comput Electron Agric 67(1â2):80â84<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1016/j.compag.2009.03.002"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Evaluation%20of%20three-dimensional%20accelerometers%20to%20monitor%20and%20classify%20behavior%20patterns%20in%20cattle&amp;author=B.%20Robert&amp;author=B.%20White&amp;author=D.%20Renter&amp;author=R.%20Larson&amp;journal=Comput%20Electron%20Agric&amp;volume=67&amp;issue=1%E2%80%932&amp;pages=80-84&amp;publication_year=2009"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">45.</div><div class="CitationContent" id="CR45">Rodgers J, Nicewander W (1988) Thirteen ways to look at the correlation coefficient. Am Stat 42(1):59â66<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.2307/2685263"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Thirteen%20ways%20to%20look%20at%20the%20correlation%20coefficient&amp;author=J.%20Rodgers&amp;author=W.%20Nicewander&amp;journal=Am%20Stat&amp;volume=42&amp;issue=1&amp;pages=59-66&amp;publication_year=1988"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">46.</div><div class="CitationContent" id="CR46">Sakoe H, Chiba S (1978) Dynamic programming algorithm optimization for spoken word recognition. IEEE Trans Acoust Speech Signal Process 26(1):43â 49<span class="Occurrences"><span class="Occurrence OccurrenceZLBID"><a class="gtm-reference" data-reference-type="MATH" target="_blank" rel="noopener" href="http://www.emis.de/MATH-item?0371.68035"><span><span>zbMATH</span></span></a></span><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/TASSP.1978.1163055"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Dynamic%20programming%20algorithm%20optimization%20for%20spoken%20word%20recognition&amp;author=H.%20Sakoe&amp;author=S.%20Chiba&amp;journal=IEEE%20Trans%20Acoust%20Speech%20Signal%20Process&amp;volume=26&amp;issue=1&amp;pages=43-49&amp;publication_year=1978"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">47.</div><div class="CitationContent" id="CR47">Santos AC, Tarrataca L, Cardoso JMP, Ferreira DR, Diniz PC, Chainho P (2009) Context inference for mobile applications in the UPCASE project. In: Proceedings of the 2nd international conference on mobile wireless middleware, operating systems, and applications (MOBILWARE 2009). Springer, no. 7 in LNICST, pp 352â365<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Santos%20AC%2C%20Tarrataca%20L%2C%20Cardoso%20JMP%2C%20Ferreira%20DR%2C%20Diniz%20PC%2C%20Chainho%20P%20%282009%29%20Context%20inference%20for%20mobile%20applications%20in%20the%20UPCASE%20project.%20In%3A%20Proceedings%20of%20the%202nd%20international%20conference%20on%20mobile%20wireless%20middleware%2C%20operating%20systems%2C%20and%20applications%20%28MOBILWARE%202009%29.%20Springer%2C%20no.%207%20in%20LNICST%2C%20pp%20352%E2%80%93365"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">48.</div><div class="CitationContent" id="CR48">Schmidt A (2002) Ubiquitous computingâcomputing in context. PhD thesis, Lancaster University, England, UK<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Schmidt%20A%20%282002%29%20Ubiquitous%20computing%E2%80%94computing%20in%20context.%20PhD%20thesis%2C%20Lancaster%20University%2C%20England%2C%20UK"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">49.</div><div class="CitationContent" id="CR49">Schmidt A, van Laerhoven K (2001) How to build smart appliances? Pers Commun IEEE 8(4):66â71<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/98.944006"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=How%20to%20build%20smart%20appliances%3F&amp;author=A.%20Schmidt&amp;author=K.%20Laerhoven&amp;journal=Pers%20Commun%20IEEE&amp;volume=8&amp;issue=4&amp;pages=66-71&amp;publication_year=2001"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">50.</div><div class="CitationContent" id="CR50">Schmidt A, Beigl M, Gellersen HW (1999a) There is more to context than location. Comput Graph 23:893â901<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1016/S0097-8493(99)00120-X"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=There%20is%20more%20to%20context%20than%20location&amp;author=A.%20Schmidt&amp;author=M.%20Beigl&amp;author=HW.%20Gellersen&amp;journal=Comput%20Graph&amp;volume=23&amp;pages=893-901&amp;publication_year=1999"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">51.</div><div class="CitationContent" id="CR51">Schmidt A, Gellersen HW, Beigl M (1999b) A wearable context-awareness component. finally a good reason to wear a tie. In: Proceedings of the 1999 international symposium on wearable computers, (ISWCâ99), pp 176â177<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Schmidt%20A%2C%20Gellersen%20HW%2C%20Beigl%20M%20%281999b%29%20A%20wearable%20context-awareness%20component.%20finally%20a%20good%20reason%20to%20wear%20a%20tie.%20In%3A%20Proceedings%20of%20the%201999%20international%20symposium%20on%20wearable%20computers%2C%20%28ISWC%E2%80%9999%29%2C%20pp%20176%E2%80%93177"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">52.</div><div class="CitationContent" id="CR52">Sekine M, Tamura T, Ogawa M, Togawa T, Fukui Y (1998) Classification of acceleration waveform in a continuous walking record. In: Proceedings of the 20th annual IEEE international conference of the engineering in medicine and biology society, vol 3, pp 1523â1526<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Sekine%20M%2C%20Tamura%20T%2C%20Ogawa%20M%2C%20Togawa%20T%2C%20Fukui%20Y%20%281998%29%20Classification%20of%20acceleration%20waveform%20in%20a%20continuous%20walking%20record.%20In%3A%20Proceedings%20of%20the%2020th%20annual%20IEEE%20international%20conference%20of%20the%20engineering%20in%20medicine%20and%20biology%20society%2C%20vol%203%2C%20pp%201523%E2%80%931526"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">53.</div><div class="CitationContent" id="CR53">Sekine M, Tamura T, Fujimoto T, Fukui Y (2000) Classification of walking pattern using acceleration waveform in elderly people. In: Proceedings of the 22nd annual IEEE international conference of the engineering in medicine and biology society, vol 2, pp 1356â1359<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Sekine%20M%2C%20Tamura%20T%2C%20Fujimoto%20T%2C%20Fukui%20Y%20%282000%29%20Classification%20of%20walking%20pattern%20using%20acceleration%20waveform%20in%20elderly%20people.%20In%3A%20Proceedings%20of%20the%2022nd%20annual%20IEEE%20international%20conference%20of%20the%20engineering%20in%20medicine%20and%20biology%20society%2C%20vol%202%2C%20pp%201356%E2%80%931359"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">54.</div><div class="CitationContent" id="CR54">Sekine M, Tamura T, Akay M, Fujimoto T, Togawa T, Fukui Y (2002) Discrimination of walking patterns using wavelet-based fractal analysis. IEEE Trans Neural Syst Rehabil Eng 10:188â196<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/TNSRE.2002.802879"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Discrimination%20of%20walking%20patterns%20using%20wavelet-based%20fractal%20analysis&amp;author=M.%20Sekine&amp;author=T.%20Tamura&amp;author=M.%20Akay&amp;author=T.%20Fujimoto&amp;author=T.%20Togawa&amp;author=Y.%20Fukui&amp;journal=IEEE%20Trans%20Neural%20Syst%20Rehabil%20Eng&amp;volume=10&amp;pages=188-196&amp;publication_year=2002"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">55.</div><div class="CitationContent" id="CR55">Siewiorek D, Smailagic A, Furukawa J, Krause A, Moraveji N, Reiger K, Shaffer J, Wong F (2003) Sensay: a context-aware mobile phone. In: Proceedings of the 7th international symposium on wearable computers (ISWC)<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Siewiorek%20D%2C%20Smailagic%20A%2C%20Furukawa%20J%2C%20Krause%20A%2C%20Moraveji%20N%2C%20Reiger%20K%2C%20Shaffer%20J%2C%20Wong%20F%20%282003%29%20Sensay%3A%20a%20context-aware%20mobile%20phone.%20In%3A%20Proceedings%20of%20the%207th%20international%20symposium%20on%20wearable%20computers%20%28ISWC%29"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">56.</div><div class="CitationContent" id="CR56">Stiefmeier T, Roggen D, TrÃ¶ster G (2007) Gestures are strings: efficient online gesture spotting and classification using string matching. In: Proceedings of international conference BodyNets 07<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Stiefmeier%20T%2C%20Roggen%20D%2C%20Tr%C3%B6ster%20G%20%282007%29%20Gestures%20are%20strings%3A%20efficient%20online%20gesture%20spotting%20and%20classification%20using%20string%20matching.%20In%3A%20Proceedings%20of%20international%20conference%20BodyNets%2007"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">57.</div><div class="CitationContent" id="CR57">Vail D, Veloso M (2004) Learning from accelerometer data on a legged robot. In: Proceedings of the 5th IFAC/EURON symposium on intelligent autonomous vehicles<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Vail%20D%2C%20Veloso%20M%20%282004%29%20Learning%20from%20accelerometer%20data%20on%20a%20legged%20robot.%20In%3A%20Proceedings%20of%20the%205th%20IFAC%2FEURON%20symposium%20on%20intelligent%20autonomous%20vehicles"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">58.</div><div class="CitationContent" id="CR58">Van Laerhoven K, Cakmakci O (2000) What shall we teach our pants. In: Proceedings of the fourth international symposium on wearable computers (ISWCâ00)<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Van%20Laerhoven%20K%2C%20Cakmakci%20O%20%282000%29%20What%20shall%20we%20teach%20our%20pants.%20In%3A%20Proceedings%20of%20the%20fourth%20international%20symposium%20on%20wearable%20computers%20%28ISWC%E2%80%9900%29"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">59.</div><div class="CitationContent" id="CR59">Van Laerhoven K, Aidoo K, Lowette S (2001) Real-time analysis of data from many sensors with neural networks. In: Proceedings of the 5th international symposium on wearable computers (ISWCâ01), pp 115â122<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Van%20Laerhoven%20K%2C%20Aidoo%20K%2C%20Lowette%20S%20%282001%29%20Real-time%20analysis%20of%20data%20from%20many%20sensors%20with%20neural%20networks.%20In%3A%20Proceedings%20of%20the%205th%20international%20symposium%20on%20wearable%20computers%20%28ISWC%E2%80%9901%29%2C%20pp%20115%E2%80%93122"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">60.</div><div class="CitationContent" id="CR60">Veltink P, Bussmann H, de Vries W, Martens W, Van Lummel R (1996) Detection of static and dynamic activities using uniaxial accelerometers. IEEE Trans Rehabil Eng 4(4):375â385<span class="Occurrences"><span class="Occurrence OccurrenceDOI"><a class="gtm-reference" data-reference-type="CrossRef" target="_blank" rel="noopener" href="https://doi.org/10.1109/86.547939"><span><span>CrossRef</span></span></a></span><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="http://scholar.google.com/scholar_lookup?title=Detection%20of%20static%20and%20dynamic%20activities%20using%20uniaxial%20accelerometers&amp;author=P.%20Veltink&amp;author=H.%20Bussmann&amp;author=W.%20Vries&amp;author=W.%20Martens&amp;author=R.%20Van%20Lummel&amp;journal=IEEE%20Trans%20Rehabil%20Eng&amp;volume=4&amp;issue=4&amp;pages=375-385&amp;publication_year=1996"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">61.</div><div class="CitationContent" id="CR61">Wang S, Pentney W, Popescu AM (2007) Common sense based joint training of human activity recognizers. In: Proceedings of the 20th international joint conference on artificial intelligence (IJCAIâ07), pp 2237â2242<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Wang%20S%2C%20Pentney%20W%2C%20Popescu%20AM%20%282007%29%20Common%20sense%20based%20joint%20training%20of%20human%20activity%20recognizers.%20In%3A%20Proceedings%20of%20the%2020th%20international%20joint%20conference%20on%20artificial%20intelligence%20%28IJCAI%E2%80%9907%29%2C%20pp%202237%E2%80%932242"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">62.</div><div class="CitationContent" id="CR62">Welbourne E, Lester J, LaMarca A, Borriello G (2007) Mobile context inference using low-cost sensors. In: Proceedings of the first international workshop on Location- and Context-Awareness (LoCA 2005), Springer, LNCS, vol 3479, pp 254â263<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Welbourne%20E%2C%20Lester%20J%2C%20LaMarca%20A%2C%20Borriello%20G%20%282007%29%20Mobile%20context%20inference%20using%20low-cost%20sensors.%20In%3A%20Proceedings%20of%20the%20first%20international%20workshop%20on%20Location-%20and%20Context-Awareness%20%28LoCA%202005%29%2C%20Springer%2C%20LNCS%2C%20vol%203479%2C%20pp%20254%E2%80%93263"><span><span>Google Scholar</span></span></a></span></span></div></li><li class="Citation"><div class="CitationNumber">63.</div><div class="CitationContent" id="CR63">Wiggins H (2008) Gesture recognition of Nintendo Wiimote input using an artificial neural network. Cognitive Systems, University of British Columbia, Vancouver, Canada<span class="Occurrences"><span class="Occurrence OccurrenceGS"><a target="_blank" rel="noopener" class="google-scholar-link gtm-reference" data-reference-type="Google Scholar" href="https://scholar.google.com/scholar?q=Wiggins%20H%20%282008%29%20Gesture%20recognition%20of%20Nintendo%20Wiimote%20input%20using%20an%20artificial%20neural%20network.%20Cognitive%20Systems%2C%20University%20of%20British%20Columbia%2C%20Vancouver%2C%20Canada"><span><span>Google Scholar</span></span></a></span></span></div></li></ol></div></section><section class="Section1 RenderAsSection1"><h2 class="Heading" id="copyrightInformation">Copyright information</h2><div class="ArticleCopyright content"><div class="ArticleCopyright">Â©Â Springer-Verlag London LimitedÂ 2010</div></div></section><section id="authorsandaffiliations" class="Section1 RenderAsSection1"><h2 class="Heading">Authors and Affiliations</h2><div class="content authors-affiliations u-interface"><ul class="test-contributor-names"><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">DavideÂ Figo</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">PedroÂ C.Â Diniz</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">DiogoÂ R.Â Ferreira</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-1">1</li></ul><span class="author-information"><span class="author-information__contact u-icon-before"><a href="mailto:diogo.ferreira@ist.utl.pt" title="diogo.ferreira@ist.utl.pt" itemprop="email" data-track="click" data-track-action="Email author" data-track-label="">Email author</a></span></span></li><li itemscope="" itemtype="http://schema.org/Person" class="u-mb-2 u-pt-4 u-pb-4"><span itemprop="name" class="authors-affiliations__name">JoÃ£oÂ M.Â P.Â Cardoso</span><ul class="authors-affiliations__indexes u-inline-list" data-role="AuthorsIndexes"><li data-affiliation="affiliation-2">2</li></ul></li></ul><ol class="test-affiliations"><li class="affiliation" data-test="affiliation-1" data-affiliation-highlight="affiliation-1" itemscope="" itemtype="http://schema.org/Organization"><span class="affiliation__count">1.</span><span class="affiliation__item"><span itemprop="name" class="affiliation__name">IST, Technical University of Lisbon</span><span itemprop="address" itemscope="" itemtype="http://schema.org/PostalAddress" class="affiliation__address"><span itemprop="addressRegion" class="affiliation__city">Porto Salvo</span><span itemprop="addressCountry" class="affiliation__country">Portugal</span></span></span></li><li class="affiliation" data-test="affiliation-2" data-affiliation-highlight="affiliation-2" itemscope="" itemtype="http://schema.org/Organization"><span class="affiliation__count">2.</span><span class="affiliation__item"><span itemprop="department" class="affiliation__department">Faculty of Engineering</span><span itemprop="name" class="affiliation__name"> University of Porto (FEUP)</span><span itemprop="address" itemscope="" itemtype="http://schema.org/PostalAddress" class="affiliation__address"><span itemprop="addressRegion" class="affiliation__city">Porto</span><span itemprop="addressCountry" class="affiliation__country">Portugal</span></span></span></li></ol></div></section></div>
                        </article>
                        <aside class="section section--collapsible" id="AboutThisContent">
    <h2 class="section__heading" id="aboutcontent">About this article</h2>
    <div class="section__content bibliographic-information">
        
        <div class="crossmark__adjacent">
            <dl class="citation-info u-highlight-target u-mb-16" id="citeas" tabindex="-1">
    <dt class="test-cite-heading">
        Cite this article as:
    </dt>
    <dd id="citethis-text">Figo, D., Diniz, P.C., Ferreira, D.R. et al. Pers Ubiquit Comput (2010) 14: 645. https://doi.org/10.1007/s00779-010-0293-9</dd>
</dl>
                <ul class="bibliographic-information__list bibliographic-information__list--inline">
        <li class="bibliographic-information__item">
            <span class="bibliographic-information__title">Received</span>
            <span class="bibliographic-information__value u-overflow-wrap">01 February 2010</span>
        </li>
        <li class="bibliographic-information__item">
            <span class="bibliographic-information__title">Accepted</span>
            <span class="bibliographic-information__value u-overflow-wrap">02 March 2010</span>
        </li>
        <li class="bibliographic-information__item">
            <span class="bibliographic-information__title">First Online</span>
            <span class="bibliographic-information__value u-overflow-wrap">30 March 2010</span>
        </li>
        <li class="bibliographic-information__item">
            <span class="bibliographic-information__title">DOI</span>
            <span class="bibliographic-information__value u-overflow-wrap" id="doi-url">https://doi.org/10.1007/s00779-010-0293-9</span>
        </li>
            <li class="bibliographic-information__item">
                <span class="bibliographic-information__title">Publisher Name</span>
                <span class="bibliographic-information__value" id="publisher-name">Springer-Verlag</span>
            </li>
            <li class="bibliographic-information__item">
                <span class="bibliographic-information__title">Print ISSN</span>
                <span class="bibliographic-information__value" id="print-issn">1617-4909</span>
            </li>
            <li class="bibliographic-information__item">
                <span class="bibliographic-information__title">Online ISSN</span>
                <span class="bibliographic-information__value" id="electronic-issn">1617-4917</span>
            </li>

        
    </ul>

            <ul class="bibliographic-information__list">
        <li class="bibliographic-information__item">
            <a id="about-journal" class="bibliographic-information__misc-links"
               title="Visit Springer.com for information about this article&#39;s journal"
               href="//www.springer.com/journal/779/about">About this journal</a>
        </li>
        <li class="bibliographic-information__item">
            <a id="reprintsandpermissions-link" target="_blank" rel="noopener" href="https://s100.copyright.com/AppDispatchServlet?publisherName&#x3D;SpringerNature&amp;orderBeanReset&#x3D;true&amp;orderSource&#x3D;SpringerLink&amp;copyright&#x3D;Springer-Verlag+London+Limited&amp;author&#x3D;Davide+Figo%2C+Pedro+C.+Diniz%2C+Diogo+R.+Ferreira+et+al&amp;issueNum&#x3D;7&amp;contentID&#x3D;10.1007%2Fs00779-010-0293-9&amp;endPage&#x3D;662&amp;publicationDate&#x3D;2010&amp;startPage&#x3D;645&amp;volumeNum&#x3D;14&amp;title&#x3D;Preprocessing+techniques+for+context+recognition+from+accelerometer+data&amp;imprint&#x3D;Springer-Verlag+London+Limited&amp;publication&#x3D;1617-4909" title="Visit RightsLink for information about reusing this article" data-track="click" data-track-action="Reprints and Permissions" data-track-label="">Reprints and Permissions</a>
        </li>
</ul>



        </div>
      
      
          
    </div>
</aside>

                        <div class="section section--collapsible uptodate-recommendations gtm-recommendations">
    <h2 class="uptodate-recommendations__title section__heading gtm-recommendations__title" id="uptodaterecommendations">Personalised recommendations</h2>
    <div class="section__content">
        <div class="uptodate-recommendations__container">
             <link rel="uptodate-inline" href="/springerlink-static/632953562/css/recommendations.css"/>
        </div>
    </div>
</div>
                                <div id="doubleclick-native-ad" data-component="SpringerLink.GoogleAds" data-namespace="native"></div>

                                    <div class="sticky-banner 
            u-interface u-js-screenreader-only" aria-hidden="true" data-component="SpringerLink.StickyBanner" data-namespace="hasButton">
                <div class="sticky-banner__container">
                        <div class="citations" data-component="SV.Dropdown" data-namespace="citationsSticky">
        <h3 class="u-h4" data-role="button-dropdown__title">
    <span>Cite</span>
    <span class="hide-text-small">article</span>
</h3>
<ul class="citations__content" data-role="button-dropdown__content">
    <li>
        <a href="#citeas" data-track="click" data-track-action="Cite as link" data-track-label="Cite dropdown">How to cite?</a>
    </li>
        <li>
            <a href="//citation-needed.springer.com/v2/references/10.1007/s00779-010-0293-9?format&#x3D;refman&amp;flavour&#x3D;citation"
               title="Download this article&#39;s citation as a .RIS file" data-track="click" data-track-action="Export citation" data-track-label="RIS">
                <span class="citations__extension" data-gtmlabel="RIS">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#004aa7"/></svg>
                    .RIS
                </span>
                <span class="citations__types">
                        <span>
                            Papers
                        </span>
                        <span>
                            Reference Manager
                        </span>
                        <span>
                            RefWorks
                        </span>
                        <span>
                            Zotero
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="//citation-needed.springer.com/v2/references/10.1007/s00779-010-0293-9?format&#x3D;endnote&amp;flavour&#x3D;citation"
               title="Download this article&#39;s citation as a .ENW file" data-track="click" data-track-action="Export citation" data-track-label="ENW">
                <span class="citations__extension" data-gtmlabel="ENW">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#004aa7"/></svg>
                    .ENW
                </span>
                <span class="citations__types">
                        <span>
                            EndNote
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="//citation-needed.springer.com/v2/references/10.1007/s00779-010-0293-9?format&#x3D;bibtex&amp;flavour&#x3D;citation"
               title="Download this article&#39;s citation as a .BIB file" data-track="click" data-track-action="Export citation" data-track-label="BIB">
                <span class="citations__extension" data-gtmlabel="BIB">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#004aa7"/></svg>
                    .BIB
                </span>
                <span class="citations__types">
                        <span>
                            BibTeX
                        </span>
                        <span>
                            JabRef
                        </span>
                        <span>
                            Mendeley
                        </span>
                </span>
            </a>
        </li>
</ul>
    </div>

                            <div>
        <a class="c-button share-this test-shareby-sharelink-link" data-test="shareable-link" target="_blank" rel="noopener" href="/sharelink/10.1007/s00779-010-0293-9" data-track="click" data-track-action="Share via" data-track-label="ShareLink">
            <span>Share</span>
            <span class="hide-text-small">article</span>
        </a>
    </div>




                                    <div>
            <a href="/content/pdf/10.1007%2Fs00779-010-0293-9.pdf" target="_blank" class="c-button c-button--blue c-button__icon-right" title="Download this article in PDF format" rel="noopener" data-track="click" data-track-action="Pdf download" data-track-label="">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" version="1.1"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill="#fff"><g transform="translate(12.000000, 5.000000)"><path d="M7 7.3L7 1C7 0.4 6.6 0 6 0 5.4 0 5 0.4 5 1L5 7.3 3.5 5.7C3.1 5.3 2.5 5.3 2.1 5.7L2.1 5.7C1.7 6.1 1.7 6.7 2.1 7.1L5.3 10.3C5.7 10.7 6.3 10.7 6.7 10.3L9.9 7.1C10.3 6.7 10.3 6.1 9.9 5.7L9.9 5.7C9.5 5.3 8.9 5.3 8.5 5.7L7 7.3 7 7.3ZM0 13C0 12.4 0.5 12 1 12L11 12C11.6 12 12 12.4 12 13 12 13.6 11.5 14 11 14L1 14C0.4 14 0 13.6 0 13L0 13Z"/></g></g></g></svg>
                <span class="hide-text-small">Download</span>
                <span>PDF</span>
            </a>
        </div>

                </div>
            </div>




                    </div>
                    <aside class="main-sidebar-right u-interface">
                        <div data-role="sticky-wrapper">
                            <div class="main-sidebar-right__content u-composite-layer" data-component="SpringerLink.StickySidebar">
                                <div class="article-actions" id="article-actions">
                                    <h2 class="u-screenreader-only" aria-hidden="true">Actions</h2>


                                    <div class="u-js-hide u-js-show-two-col">
                                        

                                                <div class="download-article test-pdf-link">
                                                            <div>
            <a href="/content/pdf/10.1007%2Fs00779-010-0293-9.pdf" target="_blank" class="c-button c-button--blue c-button__icon-right" title="Download this article in PDF format" rel="noopener" data-track="click" data-track-action="Pdf download" data-track-label="">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" version="1.1"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill="#fff"><g transform="translate(12.000000, 5.000000)"><path d="M7 7.3L7 1C7 0.4 6.6 0 6 0 5.4 0 5 0.4 5 1L5 7.3 3.5 5.7C3.1 5.3 2.5 5.3 2.1 5.7L2.1 5.7C1.7 6.1 1.7 6.7 2.1 7.1L5.3 10.3C5.7 10.7 6.3 10.7 6.7 10.3L9.9 7.1C10.3 6.7 10.3 6.1 9.9 5.7L9.9 5.7C9.5 5.3 8.9 5.3 8.5 5.7L7 7.3 7 7.3ZM0 13C0 12.4 0.5 12 1 12L11 12C11.6 12 12 12.4 12 13 12 13.6 11.5 14 11 14L1 14C0.4 14 0 13.6 0 13L0 13Z"/></g></g></g></svg>
                <span class="hide-text-small">Download</span>
                <span>PDF</span>
            </a>
        </div>

                                                </div>


                                            <div class="citations" data-component="SV.Dropdown" data-namespace="citations">
        <h3 class="u-h4" data-role="button-dropdown__title">
    <span>Cite</span>
    <span class="hide-text-small">article</span>
</h3>
<ul class="citations__content" data-role="button-dropdown__content">
    <li>
        <a href="#citeas" data-track="click" data-track-action="Cite as link" data-track-label="Cite dropdown">How to cite?</a>
    </li>
        <li>
            <a href="//citation-needed.springer.com/v2/references/10.1007/s00779-010-0293-9?format&#x3D;refman&amp;flavour&#x3D;citation"
               title="Download this article&#39;s citation as a .RIS file" data-track="click" data-track-action="Export citation" data-track-label="RIS">
                <span class="citations__extension" data-gtmlabel="RIS">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#004aa7"/></svg>
                    .RIS
                </span>
                <span class="citations__types">
                        <span>
                            Papers
                        </span>
                        <span>
                            Reference Manager
                        </span>
                        <span>
                            RefWorks
                        </span>
                        <span>
                            Zotero
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="//citation-needed.springer.com/v2/references/10.1007/s00779-010-0293-9?format&#x3D;endnote&amp;flavour&#x3D;citation"
               title="Download this article&#39;s citation as a .ENW file" data-track="click" data-track-action="Export citation" data-track-label="ENW">
                <span class="citations__extension" data-gtmlabel="ENW">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#004aa7"/></svg>
                    .ENW
                </span>
                <span class="citations__types">
                        <span>
                            EndNote
                        </span>
                </span>
            </a>
        </li>
        <li>
            <a href="//citation-needed.springer.com/v2/references/10.1007/s00779-010-0293-9?format&#x3D;bibtex&amp;flavour&#x3D;citation"
               title="Download this article&#39;s citation as a .BIB file" data-track="click" data-track-action="Export citation" data-track-label="BIB">
                <span class="citations__extension" data-gtmlabel="BIB">
                    <svg class="u-vertical-align-absolute" width="12" height="14" viewBox="0 0 12 14" xmlns="http://www.w3.org/2000/svg"><path d="M7 7.269v-6.271c0-.551-.448-.998-1-.998-.556 0-1 .447-1 .998v6.271l-1.5-1.547c-.375-.387-1.01-.397-1.401-.006l.016-.016c-.397.397-.391 1.025-.001 1.416l3.178 3.178c.392.392 1.024.391 1.415 0l3.178-3.178c.392-.392.391-1.025-.001-1.416l.016.016c-.397-.397-1.018-.388-1.401.006l-1.5 1.547zm-7 5.731c0-.552.456-1 1.002-1h9.995c.554 0 1.002.444 1.002 1 0 .552-.456 1-1.002 1h-9.995c-.554 0-1.002-.444-1.002-1z" fill="#004aa7"/></svg>
                    .BIB
                </span>
                <span class="citations__types">
                        <span>
                            BibTeX
                        </span>
                        <span>
                            JabRef
                        </span>
                        <span>
                            Mendeley
                        </span>
                </span>
            </a>
        </li>
</ul>
    </div>

                                                <div>
        <a class="c-button share-this test-shareby-sharelink-link" data-test="shareable-link" target="_blank" rel="noopener" href="/sharelink/10.1007/s00779-010-0293-9" data-track="click" data-track-action="Share via" data-track-label="ShareLink">
            <span>Share</span>
            <span class="hide-text-small">article</span>
        </a>
    </div>




                                    </div>
                                </div>
                                <nav class="toc" aria-label="article contents">
    <h2 class="u-screenreader-only" aria-hidden="true">Table of contents</h2>
    <ul id="article-contents" class="article-contents" tabindex="-1">
            <li>
                <a title="Article" href="#enumeration"><span class="u-overflow-ellipsis">Article</span></a>
            </li>
            <li>
                <a title="Abstract" href="#Abs1"><span class="u-overflow-ellipsis">Abstract</span></a>
            </li>
            <li>
                <a title="1 Introduction" href="#Sec1"><span class="u-overflow-ellipsis">1 Introduction</span></a>
            </li>
            <li>
                <a title="2 Preprocessing techniques: domains and approaches" href="#Sec2"><span class="u-overflow-ellipsis">2 Preprocessing techniques: domains and approaches</span></a>
            </li>
            <li>
                <a title="3 Experimental evaluation" href="#Sec29"><span class="u-overflow-ellipsis">3 Experimental evaluation</span></a>
            </li>
            <li>
                <a title="4 Conclusion" href="#Sec38"><span class="u-overflow-ellipsis">4 Conclusion</span></a>
            </li>
            <li>
                <a title="Footnotes" href="#Footnotes"><span class="u-overflow-ellipsis">Footnotes</span></a>
            </li>
            <li>
                <a title="References" href="#Bib1"><span class="u-overflow-ellipsis">References</span></a>
            </li>
            <li>
                <a title="Copyright information" href="#copyrightInformation"><span class="u-overflow-ellipsis">Copyright information</span></a>
            </li>
            <li>
                <a title="Authors and Affiliations" href="#authorsandaffiliations"><span class="u-overflow-ellipsis">Authors and Affiliations</span></a>
            </li>
            <li>
                <a title="About this article" href="#aboutcontent"><span class="u-overflow-ellipsis">About this article</span></a>
            </li>
    </ul>
</nav>

                            </div>
                                <div class="skyscraper-ad u-hide" data-component="SpringerLink.GoogleAds" data-namespace="skyscraper">
        <div class="skyscraper-ad__wrapper">
            <p class="skyscraper-ad__label">Advertisement</p>
            <button class="skyscraper-ad__hide" title="Hide this advertisement" data-track="click" data-track-action="Hide advertisement" data-track-label="">Hide</button>
            <div id="doubleclick-ad" class="skyscraper-ad__ad"></div>
        </div>
    </div>

                        </div>
                    </aside>
                </div>
            </main>
                <footer class="footer u-interface">
        <div class="footer__aside-wrapper">
            <div class="footer__content">
                <div class="footer__aside">
                    <p class="footer__strapline">Over 10 million scientific documents at your fingertips</p>
                                <div class="footer__edition" data-component="SV.EditionSwitcher">
                                    <h3 class="u-hide" data-role="button-dropdown__title" data-btn-text="Switch between Academic &#38; Corporate Edition">Switch Edition</h3>
                                    <ul data-role="button-dropdown__content">
                                        <li  class="selected"><a href="/siteEdition/link?previousUrl=/article/10.1007/s00779-010-0293-9&id=siteedition-academic-link" id="siteedition-academic-link">Academic Edition</a></li>
                                        <li ><a href="/siteEdition/rd?previousUrl=/article/10.1007/s00779-010-0293-9&id=siteedition-corporate-link" id="siteedition-corporate-link">Corporate Edition</a></li>
                                    </ul>
                                </div>
                </div>
            </div>
        </div>
        <div class="footer__content">
            <ul class="footer__nav">
                <li>
                    <a href="/">Home</a>
                </li>
                <li>
                    <a href="/impressum">Impressum</a>
                </li>
                <li>
                    <a href="/termsandconditions">Legal information</a>
                </li>
                <li>
                    <a href="/privacystatement">Privacy statement</a>
                </li>
                <li>
                    <a href="/cookiepolicy">How we use cookies</a>
                </li>
                <li>
                    <a class="optanon-toggle-display" href="javascript:void(0);">Cookie settings</a>
                </li>
                <li>
                    <a href="/accessibility">Accessibility</a>
                </li>
                <li>
                    <a id="contactus-footer-link" href="/contactus">Contact us</a>
                </li>
            </ul>
            <a class="parent-logo"
               target="_blank" rel="noopener"
               href="//www.springernature.com"
               title="Go to Springer Nature">
                <span class="u-screenreader-only">Springer Nature</span>
                <svg width="125" height="12" focusable="false" aria-hidden="true">
                    <image width="125" height="12"
                           src="/springerlink-static/632953562/images/png/springernature.png"
                           xmlns:xlink="http://www.w3.org/1999/xlink"
                           xlink:href="/springerlink-static/632953562/images/svg/springernature.svg">
                    </image>
                </svg>
            </a>

            <p class="footer__copyright">&copy; 2019 Springer Nature Switzerland AG. Part of <a target="_blank" rel="noopener" href="//www.springernature.com">Springer Nature</a>.</p>
                <p class="footer__user-access-info">
                    <span>Not logged in</span>
                    <span>Northwestern University (1600000022) - Northwestern University (3000118102) - Big Ten Academic Alliance Formerly (CIC) (3000133814)</span>
                    <span>165.124.85.147</span>
                </p>
        </div>
    </footer>

        </div>
        <script type="text/javascript">
    (function() {
        var linkEl = document.querySelector('.js-ctm');
        var scriptsList = [];
        var polyfillFeatures = '';

        window.SpringerLink = window.SpringerLink || {};
        window.SpringerLink.staticLocation = '/springerlink-static/632953562';
        window.eventTrackerInstance = null;

        if (window.matchMedia && window.matchMedia(linkEl.media).matches) {
            (function(h){h.className = h.className.replace('no-js', 'js')})(document.documentElement);

            polyfillFeatures = 'default,fetch,Promise,Object.setPrototypeOf,Object.entries,Number.isInteger,MutationObserver,startsWith,Array.prototype.includes,Array.from,IntersectionObserver';

            scriptsList = [
                'https://cdn.polyfill.io/v2/polyfill.min.js?features=' + polyfillFeatures + '&flags=gated',
                window.SpringerLink.staticLocation + '/js/main.js'
            ];

            scriptsList.forEach(function(script) {
                var tag = document.createElement('script');
                tag.async = false;
                tag.src = script;

                document.body.appendChild(tag);
            });
        }
    })();
</script>

    <script>
    (function() {
        var linkEl = document.querySelector('.js-ctm');
        if (window.matchMedia && window.matchMedia(linkEl.media).matches) {
            var scriptMathJax = document.createElement('script');
            scriptMathJax.async = false;
            scriptMathJax.src = '/springerlink-static/632953562/js/mathJax.js';
            var s0 = document.getElementsByTagName('script')[0];
            s0.parentNode.insertBefore(scriptMathJax, s0);
        }
    })();
</script>


    <script type="text/javascript" id="googletag-push">
        
            var adSlot = '270604982/springerlink/779/article';
        

        var definedSlots = [
                {slot: [728, 90], containerName: 'doubleclick-leaderboard-ad'},
                {slot: [160, 600], containerName: 'doubleclick-ad'},
            {slot: [2, 2], containerName: 'doubleclick-native-ad'}
        ];
    </script>


        
        <span id="chat-widget" class="u-hide"></span>
                    <noscript>
                <img aria-hidden="true" role="presentation" src="https://ssl-springer.met.vgwort.de/na/pw-vgzm.415900-10.1007-s00779-010-0293-9" width='1' height='1' alt='' />
            </noscript>

        
    </body>
</html>
